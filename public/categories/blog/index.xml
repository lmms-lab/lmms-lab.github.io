<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog on LMMs-Lab</title><link>/categories/blog/</link><description>Recent content in Blog on LMMs-Lab</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 24 Jun 2024 12:25:12 +0800</lastBuildDate><atom:link href="/categories/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Long Context Transfer from Language to Vision</title><link>/posts/longva/</link><pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate><guid>/posts/longva/</guid><description>Our paper explores the long context transfer phenomenon and validates this property on both image and video benchmarks. We propose the Long Video Assistant (LongVA) model, which can process up to 2000 frames or over 2000K visual tokens without additional complexities.</description></item><item><title>Embracing Video Evaluations with LMMs-Eval</title><link>/posts/lmms-eval-0.2/</link><pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate><guid>/posts/lmms-eval-0.2/</guid><description>We introduce a video evaluation feature to lmms-eval, supporting video model evaluations with over most popular datasets.</description></item><item><title>Accelerating the Development of Large Multimodal Models with LMMs-Eval</title><link>/posts/lmms-eval-0.1/</link><pubDate>Thu, 07 Mar 2024 00:00:00 +0000</pubDate><guid>/posts/lmms-eval-0.1/</guid><description>One command evaluation API for fast and thorough evaluation of LMMs, providing multi-faceted insights on model performance with over 40 datasets.</description></item></channel></rss>