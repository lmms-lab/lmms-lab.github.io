<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Video Models on LMMs-Lab</title><link>/tags/video-models/</link><description>Recent content in Video Models on LMMs-Lab</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 24 Jun 2024 21:08:45 +0800</lastBuildDate><atom:link href="/tags/video-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Long Context Transfer from Language to Vision</title><link>/posts/longva/</link><pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate><guid>/posts/longva/</guid><description>Our paper explores the long context transfer phenomenon and validates this property on both image and video benchmarks. We propose the Long Video Assistant (LongVA) model, which can process up to 2000 frames or over 2000K visual tokens without additional complexities.</description></item><item><title>Embracing Video Evaluations with LMMs-Eval</title><link>/posts/lmms-eval-0.2/</link><pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate><guid>/posts/lmms-eval-0.2/</guid><description>We introduce a video evaluation feature to lmms-eval, supporting video model evaluations with over most popular datasets.</description></item></channel></rss>