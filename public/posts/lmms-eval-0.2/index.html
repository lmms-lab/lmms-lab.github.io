<!doctype html><html lang=en><head><meta http-equiv=Content-Type content="text/html" charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=apple-touch-icon sizes=180x180 href=https://lmms-lab.github.io/favicon/fav.png><link rel=icon type=image/png sizes=32x32 href=https://lmms-lab.github.io/favicon/fav.png><link rel=icon type=image/png sizes=16x16 href=https://lmms-lab.github.io/favicon/fav.png><link rel=manifest href=https://lmms-lab.github.io/favicon/site.webmanifest><link rel=mask-icon href=https://lmms-lab.github.io/favicon/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><title itemprop=name>Embracing Video Evaluations with LMMs-Eval Upgrades v0.2.0 | LMMs-Lab
</title><meta name=description content><meta property="og:title" content="Embracing Video Evaluations with LMMs-Eval Upgrades v0.2.0 | LMMs-Lab"><meta name=twitter:title content="Embracing Video Evaluations with LMMs-Eval Upgrades v0.2.0 | LMMs-Lab"><meta itemprop=name content="Embracing Video Evaluations with LMMs-Eval Upgrades v0.2.0 | LMMs-Lab"><meta name=application-name content="Embracing Video Evaluations with LMMs-Eval Upgrades v0.2.0 | LMMs-Lab"><meta property="og:description" content><meta property="og:site_name" content="LMMs-Lab"><meta property="og:url" content="https://lmms-lab.github.io/posts/lmms-eval-0.2/"><meta property="og:locale" content="en"><meta property="og:image" content="/../assets/images/pages/lmms-eval-video.png"><meta property="og:image:secure_url" content="https://lmms-lab.github.io/assets/images/pages/lmms-eval-video.png"><meta property="og:type" content="article"><script>localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")</script><link rel=stylesheet href="/css/style.min.0fa2df9bfa8bb10d765f19c5264798497c0ca5d8b3d77ce0816b566d5c6fe17e.css" integrity="sha256-D6Lfm/qLsQ12XxnFJkeYSXwMpdiz13zggWtWbVxv4X4="></head><body class="bg-zinc-100 dark:bg-gray-800"><div class="top-0 z-50 w-full text-gray-200 bg-gray-900 border-2 border-gray-900 md:sticky border-b-stone-200/10"><div x-data="{ open: false }" class="flex flex-col max-w-full px-4 mx-auto md:items-center md:justify-between md:flex-row md:px-6 lg:px-8"><div class="flex flex-row items-center justify-between p-4"><a href=https://lmms-lab.github.io/ class="flex text-gray-100 transition duration-1000 ease-in-out group"><img src=https://lmms-lab.github.io/images/fav.png class="transition-opacity h-9 w-9 group-hover:opacity-50 group-focus:opacity-70" alt="LMMs-Lab Logo"><div class="mt-1 ml-3 text-xl font-black tracking-tight text-gray-100 uppercase transition-colors group-hover:text-gray-400/60">LMMs-Lab</div></a><button class="rounded-lg md:hidden focus:outline-none focus:shadow-outline" @click="open = !open" role=navigation aria-expanded=false aria-label=Main aria-controls=menuItems><svg fill="currentcolor" viewBox="0 0 20 20" class="w-6 h-6"><path x-show="!open" fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4A1 1 0 013 5zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm6 5a1 1 0 011-1h6a1 1 0 110 2h-6a1 1 0 01-1-1z" clip-rule="evenodd"/><path x-show="open" fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414.0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"/></svg></button></div><nav :class="{'flex': open, 'hidden': !open}" class="flex-col flex-grow hidden pb-4 md:pb-0 md:flex md:justify-end md:flex-row"><a class="px-4 py-2 mt-2 text-sm font-semibold rounded-lg md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=https://lmms-lab.github.io/team/ title=Team>Team
</a><a class="px-4 py-2 mt-2 text-sm font-semibold rounded-lg md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=https://lmms-lab.github.io/categories/blog/ title=Blogs>Blogs
</a><a class="px-4 py-2 mt-2 text-sm font-semibold rounded-lg md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=https://lmms-lab.github.io/news/ title=News>News</a><div @click.away="open = false" class=relative x-data="{ open: false }"><button @click="open = !open" class="flex flex-row items-center w-full px-4 py-2 mt-2 text-sm font-semibold text-left bg-transparent rounded-lg dark-mode:focus:text-white dark-mode:hover:text-white dark-mode:focus:bg-gray-600 dark-mode:hover:bg-gray-600 md:w-auto md:inline md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-600 focus:outline-none focus:shadow-outline">
<span>More</span><svg fill="currentcolor" viewBox="0 0 20 20" :class="{'rotate-180': open, 'rotate-0': !open}" class="inline w-4 h-4 mt-1 ml-1 transition-transform duration-200 transform md:-mt-1"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414.0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414.0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"/></svg></button><div x-show=open x-transition:enter="transition ease-out duration-100" x-transition:enter-start="transform opacity-0 scale-95" x-transition:enter-end="transform opacity-100 scale-100" x-transition:leave="transition ease-in duration-75" x-transition:leave-start="transform opacity-100 scale-100" x-transition:leave-end="transform opacity-0 scale-95" class="absolute right-0 z-30 w-full mt-2 origin-top-right md:max-w-sm md:w-screen"><div class="px-2 pt-2 pb-4 bg-white rounded-md shadow-lg text-zinc-900"><div class="grid grid-cols-1 gap-4"><a class="flex items-start p-2 bg-transparent rounded-lg group row hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=https://github.com/EvolvingLMMs-Lab><div class="p-3 text-white bg-primary-600 rounded-lg group-hover:bg-gray-900"><svg fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" class="w-4 h-4 md:h-6 md:w-6"><path d="M5 3v4M3 5h4M6 17v4m-2-2h4m5-16 2.286 6.857L21 12l-5.714 2.143L13 21l-2.286-6.857L5 12l5.714-2.143L13 3z"/></svg></div><div class=ml-3><p class=font-semibold>Github</p><p class=text-sm>See what we're grinding on</p></div></a><a class="flex items-start p-2 bg-transparent rounded-lg group row hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=https://huggingface.co/lmms-lab><div class="p-3 text-white bg-primary-600 rounded-lg group-hover:bg-gray-900"><svg fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" class="w-4 h-4 md:h-6 md:w-6"><path d="M8 12h.01M12 12h.01M16 12h.01M21 12c0 4.418-4.03 8-9 8a9.863 9.863.0 01-4.255-.949L3 20l1.395-3.72C3.512 15.042 3 13.574 3 12c0-4.418 4.03-8 9-8s9 3.582 9 8z"/></svg></div><div class=ml-3><p class=font-semibold>Huggingface</p><p class=text-sm>See what we're opensourcing</p></div></a><a class="flex items-start p-2 bg-transparent rounded-lg group row hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=https://github.com/LLaVA-VL/LLaVA-NeXT><div class="p-3 text-white bg-primary-600 rounded-lg group-hover:bg-gray-900"><svg fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" class="w-4 h-4 md:h-6 md:w-6"><path d="M11 3.055A9.001 9.001.0 1020.945 13H11V3.055z"/><path d="M20.488 9H15V3.512A9.025 9.025.0 0120.488 9z"/></svg></div><div class=ml-3><p class=font-semibold>Related</p><p class=text-sm>Take a look at our related projects</p></div></a></div></div></div></div></nav></div></div><article><header class="mb-4 bg-primary-600"><span class=py-96><h1 class="py-16 text-5xl font-black text-center text-white capitalize">Embracing Video Evaluations with LMMs-Eval Upgrades v0.2.0</h1></span></header><div class="max-w-4xl mx-auto mt-8 mb-2"><div class=px-6><img src=https://lmms-lab.github.io/../assets/images/pages/lmms-eval-video_hu4257d915156d31506c93852752c325fe_647392_1500x0_resize_q80_h2_box_3.webp srcset=", /../assets/images/pages/lmms-eval-video_hu4257d915156d31506c93852752c325fe_647392_400x0_resize_q80_h2_box_3.webp 400w, /../assets/images/pages/lmms-eval-video_hu4257d915156d31506c93852752c325fe_647392_550x0_resize_q80_h2_box_3.webp 550w, /../assets/images/pages/lmms-eval-video_hu4257d915156d31506c93852752c325fe_647392_900x0_resize_q80_h2_box_3.webp 768w, /../assets/images/pages/lmms-eval-video_hu4257d915156d31506c93852752c325fe_647392_1500x0_resize_q80_h2_box_3.webp 1100w" class="object-fill overflow-hidden rounded-lg shadow-lg ring-4 ring-zinc-300/40 dark:ring-gray-900/40 shadow-neutral-100/20 dark:shadow-neutral-800/40" width=100% alt></div></div><div class="max-w-4xl px-6 pt-6 pb-16 mx-auto prose dark:prose-invert dark:text-white"><p><span style=color:gray;font-size:16px><a href>Kairui Hu</a>
<a href="https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg">Kaichen Zhang</a>,
<a href=https://pufanyi.github.io/>Fanyi Pu</a>,
<a href=https://zhangyuanhan-ai.github.io/>Yuanhan Zhang</a>,
<a href=https://brianboli.com/>Bo Li</a>,
<a href=https://liuziwei7.github.io/>Ziwei Liu</a></span></p><p style=font-size:smaller><sup>*</sup> indicates equal contribution.</p><h2 id=introduction>Introduction</h2><p>In the journey towards multimodal intelligence, the development of LMMs has progressed remarkably, transitioning from handling static images to processing complex video inputs. This evolution is crucial, enabling models to understand and interpret dynamic scenes with temporal dependencies, motion dynamics, and contextual continuity. The importance of video evaluation is also increasing across various applications. However, there has been a noticeable absence of comprehensive benchmarks to evaluate the diverse array of video tasks. The introduction of <code>lmms-eval/v0.2.0</code> is both necessary and significant as it addresses this critical gap in the evaluation of video-based LMMs.</p><p>Building upon the success of <code>lmms-eval/v0.1.0</code>, <code>lmms-eval/v0.2.0</code> is a comprehensive benchmark designed specifically for video datasets. It ensures fairness by consolidating various benchmarks and evaluations into a single framework, providing consistent results across multiple datasets and tasks. This facilitates the fair comparison of different models using the same standardized benchmark. <code>lmms-eval/v0.2.0</code> is capable of running multiple datasets and tasks simultaneously, ensuring a thorough and efficient evaluation process. This framework accelerates the model iteration lifecycle, making it easier for researchers and developers to evaluate the performance of video LMMs.</p><p>By providing a fair, comprehensive, and efficient evaluation framework, <code>lmms-eval/v0.2.0</code>is set to make a significant impact on the research and development community.</p><h2 id=video-evaluation-in-lmms>Video Evaluation in LMMs</h2><h3 id=frame-extraction-for-evaluation>Frame Extraction for Evaluation</h3><p>In our framework, video evaluation can be viewed as extracting multiple frames to represent a video and then feeding the multi-frame input to the model for inference. This perspective allows us to enhance the zero-shot video understanding capability of image models by utilizing their potential to process sequences of frames as visual tokens. When each frame is considered as part of a concatenated sequence, image-only-trained models like LLaVA-Next can achieve impressive performance on video-related tasks. This highlights the significant advancement in the zero-shot video understanding capability of image models, representing a notable step forward for LMMs.</p><p>When defining these models, we also specify the number of frames to be extracted. Extracting more frames typically enhances the model&rsquo;s understanding of the entire video.</p><p>Besides, some models like Gemini and Reka do not expose the video processing interface. Our interface accommodates this by directly uploading the raw data files for evaluation.</p><h3 id=challenges-with-audio-integration>Challenges with Audio Integration</h3><p>One of the most concerning issues with these evaluations is the lack of focus on audio inputs. The majority of current evaluations overlook audio, which is a significant drawback. Audio plays a pivotal role in video content, offering supplementary context and information. At present, only the WorldQA dataset explicitly necessitates audio information to answer questions accurately. This underscores a critical gap in the evaluation process that future frameworks must address to ensure a more comprehensive evaluation of video understanding.</p><h3 id=meta-information-for-video-datasets><strong>Meta Information for Video Datasets</strong></h3><p>Table 1: Video Dataset Meta Information</p><table style=white-space:nowrap;display:flex;justify-content:center;align-items:center><tr class=bg-white-100><th class="bg-blue-100 border text-left px-8 py-4">Dataset</th><th class="bg-blue-100 border text-left px-8 py-4">Split</th><th class="bg-blue-100 border text-left px-8 py-4">Task Name</th><th class="bg-blue-100 border text-left px-8 py-4">Task Format</th><th class="bg-blue-100 border text-left px-8 py-4">Evaluation Metric</th><th class="bg-blue-100 border text-left px-8 py-4">Video Source</th><th class="bg-blue-100 border text-left px-8 py-4">Average Length</th></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">ActivityNet-QA</td><td class="border px-8 py-4">Test</td><td class="border px-8 py-4">activitynetqa</td><td class="border px-8 py-4">Open-ended</td><td class="border px-8 py-4">GPT-Eval</td><td class="border px-8 py-4">Internet</td><td class="border px-8 py-4">120s</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">EgoSchema</td><td class="border px-8 py-4">Full</td><td class="border px-8 py-4">egoschema</td><td class="border px-8 py-4">MCQ</td><td class="border px-8 py-4">Submission</td><td class="border px-8 py-4">Ego4D</td><td class="border px-8 py-4">180s</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">YouCook2</td><td class="border px-8 py-4">Validation</td><td class="border px-8 py-4">youcook2_val</td><td class="border px-8 py-4">MCQ</td><td class="border px-8 py-4" style=white-space:nowrap>Bleu; METEOR; ROUGE_L; CIDEr</td><td class="border px-8 py-4">YouTube</td><td class="border px-8 py-4">311.6s</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">Vatex</td><td class="border px-8 py-4">Test</td><td class="border px-8 py-4">vatex_test</td><td class="border px-8 py-4">Caption Matching</td><td class="border px-8 py-4">Accuracy</td><td class="border px-8 py-4">YouTube</td><td class="border px-8 py-4">147.6s</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">Vatex-ZH</td><td class="border px-8 py-4">Validation</td><td class="border px-8 py-4">vatex_val_zh</td><td class="border px-8 py-4">Caption Matching</td><td class="border px-8 py-4">Accuracy</td><td class="border px-8 py-4">YouTube</td><td class="border px-8 py-4">165.0s</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">VideoChatGPT</td><td class="border px-8 py-4">Test</td><td class="border px-8 py-4">videochatgpt</td><td class="border px-8 py-4">Open-ended</td><td class="border px-8 py-4">GPT_Eval</td><td class="border px-8 py-4">ActivityNet-200</td><td class="border px-8 py-4">120s</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">VideoDetailCaptions</td><td class="border px-8 py-4">Test</td><td class="border px-8 py-4">video_dc499</td><td class="border px-8 py-4">Open-ended</td><td class="border px-8 py-4">GPT_Eval</td><td class="border px-8 py-4">ActivityNet-200</td><td class="border px-8 py-4">120s</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">NextQA</td><td class="border px-8 py-4">OE (Text / Validation), MC (Test)</td><td class="border px-8 py-4">nextqa</td><td class="border px-8 py-4">MCQ / Open-ended</td><td class="border px-8 py-4">MC: Exact Match; OE: WUPS</td><td class="border px-8 py-4">YFCC-100M</td><td class="border px-8 py-4">44s</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">CVRR-ES</td><td class="border px-8 py-4">Default</td><td class="border px-8 py-4">cvrr</td><td class="border px-8 py-4">Open-ended</td><td class="border px-8 py-4">GPT_Eval</td><td class="border px-8 py-4">Internet; Public dataset</td><td class="border px-8 py-4">22.3s</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">Perception Test</td><td class="border px-8 py-4">MC</td><td class="border px-8 py-4">perceptiontest_val_mc</td><td class="border px-8 py-4">MCQ</td><td class="border px-8 py-4">Accuracy</td><td class="border px-8 py-4">Internet</td><td class="border px-8 py-4">23s</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">TempCompass</td><td class="border px-8 py-4">Caption Matching</td><td class="border px-8 py-4">tempcompass_caption_matching</td><td class="border px-8 py-4">Accuracy</td><td class="border px-8 py-4">Internet</td><td class="border px-8 py-4">30s</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">Video-MME</td><td class="border px-8 py-4">Test</td><td class="border px-8 py-4">videomme</td><td class="border px-8 py-4">MCQ</td><td class="border px-8 py-4">Accuracy</td><td class="border px-8 py-4">YouTube</td><td class="border px-8 py-4">1017s</td></tr></table><h3 id=alignment-check-for-video-datasets>Alignment Check for Video Datasets</h3><h2 id=more-details-and-feature-updates-with-v020>More Details and Feature Updates with <code>v0.2.0</code></h2><h3 id=improved-pipeline-for-video-evaluations><strong>Improved Pipeline for Video Evaluations</strong></h3><p>Here’s a breakdown of adding video datasets support, especially on how we implement the process from video caching, loading and feed to model to get response.</p><ol><li><p><strong>Download and Load Videos:</strong> Video are being loaded during generation phase. We will host different video datasets on the huggingface and preprocess the video path for you in your huggingface cache folder. It is recommended to set <code>HF_HOME</code> before you use our evaluation suite so that you can manage the download place. After downloading the videos from huggingface hub, we unzip them into a local cache dir, where by default is <code>HF_HOME</code>.</p><ul><li><p>The code specifically demonstrates the logic of how we handle video datasets in lmms-eval.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#a6e22e>@retry</span>(stop<span style=color:#f92672>=</span>(stop_after_attempt(<span style=color:#ae81ff>5</span>) <span style=color:#f92672>|</span> stop_after_delay(<span style=color:#ae81ff>60</span>)), wait<span style=color:#f92672>=</span>wait_fixed(<span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>download</span>(self, dataset_kwargs<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># If the dataset is a video dataset,</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Recursively search whether their is a zip and unzip it to the huggingface home</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> dataset_kwargs <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span> <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;video&#34;</span> <span style=color:#f92672>in</span> dataset_kwargs <span style=color:#f92672>and</span> dataset_kwargs[<span style=color:#e6db74>&#34;video&#34;</span>]:
</span></span><span style=display:flex><span>            hf_home <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;HF_HOME&#34;</span>, <span style=color:#e6db74>&#34;~/.cache/huggingface/&#34;</span>)
</span></span><span style=display:flex><span>            cache_dir <span style=color:#f92672>=</span> dataset_kwargs[<span style=color:#e6db74>&#34;cache_dir&#34;</span>]
</span></span><span style=display:flex><span>            cache_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(hf_home, cache_dir)
</span></span><span style=display:flex><span>            accelerator <span style=color:#f92672>=</span> Accelerator()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> accelerator<span style=color:#f92672>.</span>is_main_process:
</span></span><span style=display:flex><span>                cache_path <span style=color:#f92672>=</span> snapshot_download(repo_id<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>DATASET_PATH, repo_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;dataset&#34;</span>)
</span></span><span style=display:flex><span>                zip_files <span style=color:#f92672>=</span> glob(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(cache_path, <span style=color:#e6db74>&#34;**/*.zip&#34;</span>), recursive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(cache_dir) <span style=color:#f92672>and</span> len(zip_files) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>for</span> zip_file <span style=color:#f92672>in</span> zip_files:
</span></span><span style=display:flex><span>                        eval_logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Unzipping </span><span style=color:#e6db74>{</span>zip_file<span style=color:#e6db74>}</span><span style=color:#e6db74> to </span><span style=color:#e6db74>{</span>cache_dir<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                        shutil<span style=color:#f92672>.</span>unpack_archive(zip_file, cache_dir)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            accelerator<span style=color:#f92672>.</span>wait_for_everyone()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;builder_script&#34;</span> <span style=color:#f92672>in</span> dataset_kwargs:
</span></span><span style=display:flex><span>                builder_script <span style=color:#f92672>=</span> dataset_kwargs[<span style=color:#e6db74>&#34;builder_script&#34;</span>]
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>DATASET_PATH <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(cache_path, builder_script)
</span></span><span style=display:flex><span>                dataset_kwargs<span style=color:#f92672>.</span>pop(<span style=color:#e6db74>&#34;builder_script&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            dataset_kwargs<span style=color:#f92672>.</span>pop(<span style=color:#e6db74>&#34;cache_dir&#34;</span>)
</span></span><span style=display:flex><span>            dataset_kwargs<span style=color:#f92672>.</span>pop(<span style=color:#e6db74>&#34;video&#34;</span>)
</span></span></code></pre></div></li></ul></li><li><p><strong>Format questions:</strong> For each task, questions are formatted in the <code>&lt;taskname>/utils.py</code> file. We parse each document from the Huggingface dataset, retrieve the questions, and formulate the input with any specified model-specific prompts.</p><ul><li><p>The code specifically demonstrates the logic of how to implement question format.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># This is the place where you format your question</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>perceptiontest_doc_to_text</span>(doc, model_specific_prompt_kwargs<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> model_specific_prompt_kwargs <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        model_specific_prompt_kwargs <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    pre_prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    post_prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;pre_prompt&#34;</span> <span style=color:#f92672>in</span> model_specific_prompt_kwargs:
</span></span><span style=display:flex><span>        pre_prompt <span style=color:#f92672>=</span> model_specific_prompt_kwargs[<span style=color:#e6db74>&#34;pre_prompt&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;post_prompt&#34;</span> <span style=color:#f92672>in</span> model_specific_prompt_kwargs:
</span></span><span style=display:flex><span>        post_prompt <span style=color:#f92672>=</span> model_specific_prompt_kwargs[<span style=color:#e6db74>&#34;post_prompt&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    question <span style=color:#f92672>=</span> doc[<span style=color:#e6db74>&#34;question&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;options&#34;</span> <span style=color:#f92672>in</span> doc:
</span></span><span style=display:flex><span>        index <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> op <span style=color:#f92672>in</span> doc[<span style=color:#e6db74>&#34;options&#34;</span>]:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> index <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                question <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;A. &#34;</span> <span style=color:#f92672>+</span> op
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>elif</span> index <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>                question <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;B. &#34;</span> <span style=color:#f92672>+</span> op
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                question <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;C. &#34;</span> <span style=color:#f92672>+</span> op
</span></span><span style=display:flex><span>            index <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        post_prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Answer with the option&#39;s letter from the given choices directly.&#34;</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;question</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(question)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>pre_prompt<span style=color:#e6db74>}{</span>question<span style=color:#e6db74>}{</span>post_prompt<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div></li></ul></li><li><p><strong>Process results:</strong> After the model generates results, each result is parsed and evaluated based on the corresponding evaluation metric. The choice of metric is based on the dataset’s official implementation on their official project website. We primarily use three types of metrics:</p><p><strong>a. Accuracy:</strong>
For datasets with ground truth answers, we generate a score by comparing the model’s results with the ground truth. This metric is commonly used in multiple-choice QA tasks such as PerceptionTest-val and EgoSchema-subset.</p><p><strong>b. GPT Evaluation:</strong>
For open-ended answers generated by the model, we apply OpenAI GPT API to evaluate the responses. This metric is often used in generation tasks like ActivityNetQA and VideoChatGPT.</p><p><strong>c. Submission:</strong>
If the dataset does not provide ground truth answers and requires submission of inference results to a server for evaluation, we provide a submission file according to the dataset&rsquo;s official template. This metric is used in tasks like EgoSchema, Perception Test.</p></li><li><p><strong>Aggregate results:</strong>
After evaluating each data instance, we aggregate the individual results to generate the overall evaluation metrics. Finally, we provide a summary table that consolidates all the evaluation results, similar to the one in Google’s Gemini report.</p></li><li><p><strong>Grouped Tasks:</strong>
For tasks with multiple subsets, we group all subset tasks together. For example, the VideoChatGPT dataset includes three subsets: generic, temporal, and consistency. By running <code>--task videochatgpt</code>, all three subsets can be evaluated together, eliminating the need to specify each subset individually. We summarize all the grouped task names in Table 1. This pipeline ensures a thorough and standardized evaluation process for video LMMs, facilitating consistent and reliable performance assessment across various tasks and datasets. - This code denotes how we organize the group of tasks together.
`yaml
group: videochatgpt
task:</p><ul><li>videochatgpt_gen</li><li>videochatgpt_temporal</li><li>videochatgpt_consistency
`</li></ul></li></ol><h3 id=supported-video-tasks><strong>Supported Video Tasks</strong></h3><ol><li>ActivityNet-QA</li><li>EgoSchema</li><li>YouCook2</li><li>VATEX</li><li>VATEX-ZH</li><li>VideoChatGPT</li><li>VideoDetailCaptions</li><li>NextQA</li><li>CVRR-ES</li><li>Perception Test</li><li>TempCompass</li><li>Video-MME</li></ol><h3 id=support-video-models><strong>Support Video Models</strong></h3><p>We have supported more video models that can be used in LMMs-Eval. We now support evaluating video datasets using a one line command.</p><ol><li>LLaVA-NeXT-Video</li><li>Video-LLaVA</li><li>LLaMA-VID</li><li>Video-ChatGPT</li><li>MPLUG-OWL</li></ol><h3 id=more-updates><strong>More Updates</strong></h3><ol><li><p>For newly added tasks with different splits and metrics, we have adopted a naming rule in the format <code>{name}_{split}_{metric}</code>. For instance, <code>perceptiontest_val_mcppl</code> refers to the validation split of the PerceptionTest evaluation dataset, using multiple choice perplexity as the evaluation metric.</p></li><li><p>We support <code>llava-next</code> series models with sglang. You can use the following command to launch evaluation with sglang support, that’s much more efficient when running on <code>llava-next-72b/110b</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 -m lmms_eval <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--model<span style=color:#f92672>=</span>llava_sglang <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--model_args<span style=color:#f92672>=</span>pretrained<span style=color:#f92672>=</span>lmms-lab/llava-next-72b,tokenizer<span style=color:#f92672>=</span>lmms-lab/llavanext-qwen-tokenizer,conv_template<span style=color:#f92672>=</span>chatml-llava,tp_size<span style=color:#f92672>=</span>8,parallel<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--tasks<span style=color:#f92672>=</span>mme <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--log_samples <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--log_samples_suffix<span style=color:#f92672>=</span>llava_qwen <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--output_path<span style=color:#f92672>=</span>./logs/ <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--verbosity<span style=color:#f92672>=</span>INFO
</span></span></code></pre></div></li><li><p>We add a <code>force_download</code> mode to robustly handle the case that videos are not fully cached in local folder. You could add the args to task yaml file as the following commands. To support the evaluations that in machines that do not have access to internet, we add the <code>local_files_only</code> to support this feature.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>dataset_path</span>: <span style=color:#ae81ff>lmms-lab/ActivityNetQA</span>
</span></span><span style=display:flex><span><span style=color:#f92672>dataset_kwargs</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>token</span>: <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>video</span>: <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>force_download</span>: <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>local_files_only</span>: <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>cache_dir</span>: <span style=color:#ae81ff>activitynetqa</span>
</span></span><span style=display:flex><span><span style=color:#f92672>model_specific_prompt_kwargs</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>default</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>pre_prompt</span>: <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>post_prompt</span>: <span style=color:#e6db74>&#34; Answer the question using a single word or phrase.&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>version</span>: <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>gpt_eval_model_name</span>: <span style=color:#ae81ff>gpt-3.5-turbo-0613</span>
</span></span></code></pre></div></li><li><p>We found that sometimes the dataset downloading process will throw <code>Retry</code> or <code>HTTP Timedout</code> errors. To prevent this, we recommend disabling <code>hf_transfer</code> mechanism by setting this in your environment <code>export HF_HUB_ENABLE_HF_TRANSFER="0"</code></p></li><li><p>mmmu_group_img_val</p><p>We aligned the results of LLaVA-NeXT 34B by combining images into one. That is, in multi-image testing, <strong>all images are stitched together into one</strong>, and tested as a single image. When tested separately (<code>mmmu_val</code>), the score was 46.7, and after combination (<code>mmmu_group_img_val</code>), the score was 50.1.</p><p>Example:</p></li></ol></div></article><script defer src=https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js></script><script src=https://lmms-lab.github.io/js/darkmode.js defer></script><footer class=bg-gray-900><div class="max-w-md px-4 py-12 mx-auto overflow-hidden sm:max-w-3xl sm:px-6 lg:max-w-7xl lg:px-8"><nav class="flex flex-wrap justify-center -mx-5 -my-2" aria-label=Footer><div class="px-5 py-2"><a href=https://lmms-lab.github.io/team/ class="text-base text-gray-400 hover:text-gray-300">Team</a></div><div class="px-5 py-2"><a href=https://lmms-lab.github.io/news/ class="text-base text-gray-400 hover:text-gray-300">News</a></div><div class="px-5 py-2"><a href=https://lmms-lab.github.io/categories/blog/ class="text-base text-gray-400 hover:text-gray-300">Blog</a></div></nav><div class="flex justify-center mt-8 space-x-6"><a href=# class="text-gray-400 hover:text-gray-300"><span class=sr-only>Twitter</span>
<svg class="w-6 h-6" fill="currentcolor" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.29 20.251c7.547.0 11.675-6.253 11.675-11.675.0-.178.0-.355-.012-.53A8.348 8.348.0 0022 5.92a8.19 8.19.0 01-2.357.646 4.118 4.118.0 001.804-2.27 8.224 8.224.0 01-2.605.996 4.107 4.107.0 00-6.993 3.743A11.65 11.65.0 013.392 4.748a4.106 4.106.0 001.27 5.477A4.072 4.072.0 012.8 9.713v.052a4.105 4.105.0 003.292 4.022 4.095 4.095.0 01-1.853.07 4.108 4.108.0 003.834 2.85A8.233 8.233.0 012 18.407a11.616 11.616.0 006.29 1.84"/></svg>
</a><a href=# class="text-gray-400 hover:text-gray-300"><span class=sr-only>GitHub</span><svg class="w-6 h-6" fill="currentcolor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483.0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951.0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65.0.0.84-.27 2.75 1.026A9.564 9.564.0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688.0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855.0 1.338-.012 2.419-.012 2.747.0.268.18.58.688.482A10.019 10.019.0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"/></svg></a></div><p class="mt-8 text-base text-center text-gray-400">&copy; 2024
LMMs-Lab. All rights
reserved.</p><p class="mt-2 text-base text-center text-gray-400">Made with &#x2764;&#xfe0f; by <a href=https://nusserstudios.com class="hover:underline hover:text-primary-600"><span class="font-black uppercase">Nusser</span> <span class="font-light uppercase">Studios.</span></a></p></div></footer></body></html>