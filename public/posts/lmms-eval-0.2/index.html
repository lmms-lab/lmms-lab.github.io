<!doctype html><html lang=en><head><meta http-equiv=Content-Type content="text/html" charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=apple-touch-icon sizes=180x180 href=/favicon/fav.png><link rel=icon type=image/png sizes=32x32 href=/favicon/fav.png><link rel=icon type=image/png sizes=16x16 href=/favicon/fav.png><link rel=manifest href=/favicon/site.webmanifest><link rel=mask-icon href=/favicon/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><title itemprop=name>Embracing Video Evaluations with LMMs-Eval | LMMs-Lab
</title><meta name=description content><meta property="og:title" content="Embracing Video Evaluations with LMMs-Eval | LMMs-Lab"><meta name=twitter:title content="Embracing Video Evaluations with LMMs-Eval | LMMs-Lab"><meta itemprop=name content="Embracing Video Evaluations with LMMs-Eval | LMMs-Lab"><meta name=application-name content="Embracing Video Evaluations with LMMs-Eval | LMMs-Lab"><meta property="og:description" content><meta property="og:site_name" content="LMMs-Lab"><meta property="og:url" content="/posts/lmms-eval-0.2/"><meta property="og:locale" content="en"><meta property="og:image" content="/../assets/images/pages/lmms-eval-video.png"><meta property="og:image:secure_url" content="/assets/images/pages/lmms-eval-video.png"><meta property="og:type" content="article"><script>localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")</script><link rel=stylesheet href="/css/style.min.4d6c23186ccd4a17f2d2abf5ec34558f8455dc7617799e80879f04b20036f4e9.css" integrity="sha256-TWwjGGzNShfy0qv17DRVj4RV3HYXeZ6Ah58EsgA29Ok="></head><body class="bg-zinc-100 dark:bg-gray-800"><div class="top-0 z-50 w-full text-gray-200 bg-gray-900 border-2 border-gray-900 md:sticky border-b-stone-200/10"><div x-data="{ open: false }" class="flex flex-col max-w-full px-4 mx-auto md:items-center md:justify-between md:flex-row md:px-6 lg:px-8"><div class="flex flex-row items-center justify-between p-4"><a href=/ class="flex text-gray-100 transition duration-1000 ease-in-out group"><img src=/images/fav.png class="transition-opacity h-9 w-9 group-hover:opacity-50 group-focus:opacity-70" alt="LMMs-Lab Logo"><div class="mt-1 ml-3 text-xl font-black tracking-tight text-gray-100 uppercase transition-colors group-hover:text-gray-400/60">LMMs-Lab</div></a><button class="rounded-lg md:hidden focus:outline-none focus:shadow-outline" @click="open = !open" role=navigation aria-expanded=false aria-label=Main aria-controls=menuItems><svg fill="currentcolor" viewBox="0 0 20 20" class="w-6 h-6"><path x-show="!open" fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4A1 1 0 013 5zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm6 5a1 1 0 011-1h6a1 1 0 110 2h-6a1 1 0 01-1-1z" clip-rule="evenodd"/><path x-show="open" fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414.0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"/></svg></button></div><nav :class="{'flex': open, 'hidden': !open}" class="flex-col flex-grow hidden pb-4 md:pb-0 md:flex md:justify-end md:flex-row"><a class="px-4 py-2 mt-2 text-sm font-semibold rounded-lg md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=/team/ title=Team>Team
</a><a class="px-4 py-2 mt-2 text-sm font-semibold rounded-lg md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=/categories/blog/ title=Blogs>Blogs
</a><a class="px-4 py-2 mt-2 text-sm font-semibold rounded-lg md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=/news/ title=News>News</a><div @click.away="open = false" class=relative x-data="{ open: false }"><button @click="open = !open" class="flex flex-row items-center w-full px-4 py-2 mt-2 text-sm font-semibold text-left bg-transparent rounded-lg dark-mode:focus:text-white dark-mode:hover:text-white dark-mode:focus:bg-gray-600 dark-mode:hover:bg-gray-600 md:w-auto md:inline md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-600 focus:outline-none focus:shadow-outline">
<span>More</span><svg fill="currentcolor" viewBox="0 0 20 20" :class="{'rotate-180': open, 'rotate-0': !open}" class="inline w-4 h-4 mt-1 ml-1 transition-transform duration-200 transform md:-mt-1"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414.0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414.0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"/></svg></button><div x-show=open x-transition:enter="transition ease-out duration-100" x-transition:enter-start="transform opacity-0 scale-95" x-transition:enter-end="transform opacity-100 scale-100" x-transition:leave="transition ease-in duration-75" x-transition:leave-start="transform opacity-100 scale-100" x-transition:leave-end="transform opacity-0 scale-95" class="absolute right-0 z-30 w-full mt-2 origin-top-right md:max-w-sm md:w-screen"><div class="px-2 pt-2 pb-4 bg-white rounded-md shadow-lg text-zinc-900"><div class="grid grid-cols-1 gap-4"><a class="flex items-start p-2 bg-transparent rounded-lg group row hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=https://github.com/EvolvingLMMs-Lab><div class="p-3 text-white bg-primary-600 rounded-lg group-hover:bg-gray-900"><svg fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" class="w-4 h-4 md:h-6 md:w-6"><path d="M5 3v4M3 5h4M6 17v4m-2-2h4m5-16 2.286 6.857L21 12l-5.714 2.143L13 21l-2.286-6.857L5 12l5.714-2.143L13 3z"/></svg></div><div class=ml-3><p class=font-semibold>Github</p><p class=text-sm>See what we're grinding on</p></div></a><a class="flex items-start p-2 bg-transparent rounded-lg group row hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=https://huggingface.co/lmms-lab><div class="p-3 text-white bg-primary-600 rounded-lg group-hover:bg-gray-900"><svg fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" class="w-4 h-4 md:h-6 md:w-6"><path d="M8 12h.01M12 12h.01M16 12h.01M21 12c0 4.418-4.03 8-9 8a9.863 9.863.0 01-4.255-.949L3 20l1.395-3.72C3.512 15.042 3 13.574 3 12c0-4.418 4.03-8 9-8s9 3.582 9 8z"/></svg></div><div class=ml-3><p class=font-semibold>Huggingface</p><p class=text-sm>See what we're opensourcing</p></div></a><a class="flex items-start p-2 bg-transparent rounded-lg group row hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline" href=https://github.com/LLaVA-VL/LLaVA-NeXT><div class="p-3 text-white bg-primary-600 rounded-lg group-hover:bg-gray-900"><svg fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" class="w-4 h-4 md:h-6 md:w-6"><path d="M11 3.055A9.001 9.001.0 1020.945 13H11V3.055z"/><path d="M20.488 9H15V3.512A9.025 9.025.0 0120.488 9z"/></svg></div><div class=ml-3><p class=font-semibold>Related</p><p class=text-sm>Take a look at our related projects</p></div></a></div></div></div></div></nav></div></div><article><header class="mb-4 bg-primary-600"><span class=py-96><h1 class="py-16 text-5xl font-black text-center text-white capitalize">Embracing Video Evaluations with LMMs-Eval</h1></span></header><div class="max-w-4xl mx-auto mt-8 mb-2"><div class=px-6><img src=/../assets/images/pages/lmms-eval-video_hu4257d915156d31506c93852752c325fe_647392_1500x0_resize_q80_h2_box_3.webp srcset=", /../assets/images/pages/lmms-eval-video_hu4257d915156d31506c93852752c325fe_647392_400x0_resize_q80_h2_box_3.webp 400w, /../assets/images/pages/lmms-eval-video_hu4257d915156d31506c93852752c325fe_647392_550x0_resize_q80_h2_box_3.webp 550w, /../assets/images/pages/lmms-eval-video_hu4257d915156d31506c93852752c325fe_647392_900x0_resize_q80_h2_box_3.webp 768w, /../assets/images/pages/lmms-eval-video_hu4257d915156d31506c93852752c325fe_647392_1500x0_resize_q80_h2_box_3.webp 1100w" class="object-fill overflow-hidden rounded-lg shadow-lg ring-4 ring-zinc-300/40 dark:ring-gray-900/40 shadow-neutral-100/20 dark:shadow-neutral-800/40" width=100% alt></div></div><div class="max-w-4xl px-6 pt-6 pb-16 mx-auto prose dark:prose-invert dark:text-white"><p style=font-size:16px;line-height:1.4><span style=color:gray;font-size:18px><a href=https://kairuihu.github.io/>Kairui Hu<sup>*</sup></a>, &nbsp;
<a href=https://pufanyi.github.io/>Fanyi Pu<sup>*</sup></a>, &nbsp;
<a href="https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg">Kaichen Zhang<sup>*</sup></a>, &nbsp;
<a href=https://github.com/choiszt>Shuai Liu<sup>*</sup></a>, &nbsp;
<a href=https://zhangyuanhan-ai.github.io/>Yuanhan Zhang<sup>*</sup></a> &nbsp;<br><a href=https://brianboli.com/>Bo Li<sup>*;&#8224;</sup></a>, &nbsp;
<a href=https://veiled-texture-20c.notion.site/Peiyuan-Zhang-ab24b48621c9491db767a76df860873a>Peiyuan Zhang</a>, &nbsp;
<a href=https://liuziwei7.github.io/>Ziwei Liu</a></span></p><p style=font-size:16px;line-height:1.2>Nanyang Technical University, Singapore<br><br><sup>*</sup>indicates equal contribution.
<sup>&#8224;</sup>development lead.</p><h2 id=table-of-contents>Table of Contents</h2><ul style=color:gray;font-size:14px><li><a href=#introduction>Introduction</a></li><li><a href=#video-evaluation-in-lmms>Video Evaluation in LMMs</a><ul><li><a href=#frame-extraction-for-evaluation>Frame Extraction for Evaluation</a></li><li><a href=#challenges-with-audio-integration>Challenges with Audio Integration</a></li><li><a href=#meta-information-for-video-datasets>Meta Information for Video Datasets</a></li><li><a href=#alignment-check-for-video-datasets>Alignment Check for Video Datasets</a></li></ul></li><li><a href=#more-details-and-feature-updates-with-v020>More Details and Feature Updates with <code>v0.2.0</code></a><ul><li><a href=#improved-pipeline-for-video-evaluations>Improved Pipeline for Video Evaluations</a></li><li><a href=#improved-overall-evaluation-pipeline>Improved Overall Evaluation Pipeline</a></li><li><a href=#supported-video-tasks>Supported Video Tasks</a></li><li><a href=#supported-video-models>Supported Video Models</a></li></ul></li></ul><h2 id=introduction>Introduction</h2><p>In the journey towards multimodal intelligence, the development of LMMs has progressed remarkably, transitioning from handling static images to processing complex video inputs. This evolution is crucial, enabling models to understand and interpret dynamic scenes with temporal dependencies, motion dynamics, and contextual continuity. The importance of video evaluation is also increasing across various applications. However, there has been a noticeable absence of comprehensive benchmarks to evaluate the diverse array of video tasks. The introduction of <code>lmms-eval/v0.2.0</code> is both necessary and significant as it addresses this critical gap in the evaluation of video-based LMMs.</p><p>Building upon the success of <code>lmms-eval/v0.1.0</code>, <code>lmms-eval/v0.2.0</code> makes major upgrades on incorporating video tasks and models, and more feature updates on improved pipelines for both image and video tasks, more image models, and fixed previous community issues.</p><h2 id=video-evaluation-in-lmms>Video Evaluation in LMMs</h2><h3 id=frame-extraction-for-evaluation>Frame Extraction for Evaluation</h3><p>In our framework, video evaluation can be viewed as extracting multiple frames to represent a video and then feeding the multi-frame input to the model for inference. This perspective allows us to enhance the zero-shot video understanding capability of image models by utilizing their potential to process sequences of frames as visual tokens. When each frame is considered as part of a concatenated sequence, image-only-trained models like LLaVA-Next can achieve impressive performance on video-related tasks. This highlights the significant advancement in the zero-shot video understanding capability of image models, representing a notable step forward for LMMs.</p><p>When defining these models, we also specify the number of frames to be extracted. Extracting more frames typically enhances the model&rsquo;s understanding of the entire video.</p><p>Besides, some models like Gemini and Reka do not expose the video processing interface. Our interface accommodates this by directly uploading the raw data files for evaluation.</p><h3 id=challenges-with-audio-integration>Challenges with Audio Integration</h3><p>One of the most concerning issues with these evaluations is the lack of focus on audio inputs. The majority of current evaluations overlook audio, which is a significant drawback. Audio plays a pivotal role in video content, offering supplementary context and information. At present, only the WorldQA dataset explicitly necessitates audio information to answer questions accurately. This underscores a critical gap in the evaluation process that future frameworks must address to ensure a more comprehensive evaluation of video understanding.</p><h3 id=meta-information-for-video-datasets><strong>Meta Information for Video Datasets</strong></h3><p>Table 1: Video Dataset Meta Information</p><table style=white-space:nowrap;display:flex;justify-content:center;align-items:center;font-size:.85em><tr class=bg-white-100><th class="bg-blue-100 border text-left" style="padding:16px .75em">Dataset</th><th class="bg-blue-100 border text-left" style="padding:16px .75em">Split</th><th class="bg-blue-100 border text-left" style="padding:16px .75em">Task Name</th><th class="bg-blue-100 border text-left" style="padding:16px .75em">Task Format</th><th class="bg-blue-100 border text-left" style="padding:16px .75em">Evaluation Metric</th><th class="bg-blue-100 border text-left" style="padding:16px .75em">Video Source</th><th class="bg-blue-100 border text-left" style="padding:16px .75em">Average Length</th></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">ActivityNet-QA</td><td class=border style="padding:16px .75em">Test</td><td class=border style="padding:16px .75em">activitynetqa</td><td class=border style="padding:16px .75em">Open-ended</td><td class=border style="padding:16px .75em">GPT-Eval</td><td class=border style="padding:16px .75em">Internet</td><td class=border style="padding:16px .75em">117.3s</td></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">EgoSchema</td><td class=border style="padding:16px .75em">Full</td><td class=border style="padding:16px .75em">egoschema</td><td class=border style="padding:16px .75em">MCQ</td><td class=border style="padding:16px .75em">Submission</td><td class=border style="padding:16px .75em">Ego4D</td><td class=border style="padding:16px .75em">180s</td></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">YouCook2</td><td class=border style="padding:16px .75em">Validation</td><td class=border style="padding:16px .75em">youcook2_val</td><td class=border style="padding:16px .75em">MCQ</td><td class=border style="padding:16px .75em">Bleu; METEOR; ROUGE_L; CIDEr</td><td class=border style="padding:16px .75em">YouTube</td><td class=border style="padding:16px .75em">311.6s</td></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">Vatex</td><td class=border style="padding:16px .75em">Test</td><td class=border style="padding:16px .75em">vatex_test</td><td class=border style="padding:16px .75em">Caption Matching</td><td class=border style="padding:16px .75em">Bleu; METEOR; ROUGE_L; CIDEr</td><td class=border style="padding:16px .75em">YouTube</td><td class=border style="padding:16px .75em">147.6s</td></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">Vatex-ZH</td><td class=border style="padding:16px .75em">Validation</td><td class=border style="padding:16px .75em">vatex_val_zh</td><td class=border style="padding:16px .75em">Caption Matching</td><td class=border style="padding:16px .75em">Bleu; METEOR; ROUGE_L; CIDEr</td><td class=border style="padding:16px .75em">YouTube</td><td class=border style="padding:16px .75em">165s</td></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">VideoChatGPT</td><td class=border style="padding:16px .75em">Test</td><td class=border style="padding:16px .75em">videochatgpt</td><td class=border style="padding:16px .75em">Open-ended</td><td class=border style="padding:16px .75em">GPT_Eval</td><td class=border style="padding:16px .75em">ActivityNet-200</td><td class=border style="padding:16px .75em">108s</td></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">VideoDetailCaptions</td><td class=border style="padding:16px .75em">Test</td><td class=border style="padding:16px .75em">video_dc499</td><td class=border style="padding:16px .75em">Open-ended</td><td class=border style="padding:16px .75em">GPT_Eval</td><td class=border style="padding:16px .75em">ActivityNet-200</td><td class=border style="padding:16px .75em">108s</td></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">NextQA</td><td class=border style="padding:16px .75em">OE (Text / Validation), MC (Test)</td><td class=border style="padding:16px .75em">nextqa</td><td class=border style="padding:16px .75em">MCQ / Open-ended</td><td class=border style="padding:16px .75em">MC: Exact Match; OE: WUPS</td><td class=border style="padding:16px .75em">YFCC-100M</td><td class=border style="padding:16px .75em">44s</td></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">CVRR-ES</td><td class=border style="padding:16px .75em">Default</td><td class=border style="padding:16px .75em">cvrr</td><td class=border style="padding:16px .75em">Open-ended</td><td class=border style="padding:16px .75em">GPT_Eval</td><td class=border style="padding:16px .75em">Internet; Public dataset</td><td class=border style="padding:16px .75em">22.3s</td></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">Perception Test</td><td class=border style="padding:16px .75em">MC</td><td class=border style="padding:16px .75em">perceptiontest_val_mc</td><td class=border style="padding:16px .75em">MCQ</td><td class=border style="padding:16px .75em">Accuracy</td><td class=border style="padding:16px .75em">Internet</td><td class=border style="padding:16px .75em">23s</td></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">TempCompass</td><td class=border style="padding:16px .75em">Default</td><td class=border style="padding:16px .75em">tempcompass</td><td class=border style="padding:16px .75em">MCQ; Y/N; Captioning; Caption Matching</td><td class=border style="padding:16px .75em">Accuracy</td><td class=border style="padding:16px .75em">Internet</td><td class=border style="padding:16px .75em">11.9s</td></tr><tr class=hover:bg-gray-50><td class=border style="padding:16px .75em">Video-MME</td><td class=border style="padding:16px .75em">Test</td><td class=border style="padding:16px .75em">videomme</td><td class=border style="padding:16px .75em">MCQ</td><td class=border style="padding:16px .75em">Accuracy</td><td class=border style="padding:16px .75em">YouTube</td><td class=border style="padding:16px .75em">1017s</td></tr></table><h3 id=alignment-check-for-video-datasets>Alignment Check for Video Datasets</h3><p>Table 2. Alignment Check for Video Datasets</p><table style=white-space:nowrap;display:flex;justify-content:center;align-items:center><tr class=bg-white-100><th class="bg-blue-100 border text-left px-8 py-4">Dataset</th><th class="bg-blue-100 border text-left px-8 py-4">Subset</th><th class="bg-blue-100 border text-left px-8 py-4">Model</th><th class="bg-blue-100 border text-left px-8 py-4">Original Reported</th><th class="bg-blue-100 border text-left px-8 py-4">LMMs-Eval</th></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">EgoSchema(0-shot)</td><td class="border px-8 py-4">egoschema_subset_mc_ppl</td><td class="border px-8 py-4">LLaVA-NeXT-Video-7B</td><td class="border px-8 py-4">-</td><td class="border px-8 py-4">50.60%</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">CVRR-ES</td><td class="border px-8 py-4">cvrr_multiple_actions_in_a_single_video</td><td class="border px-8 py-4">Video-ChatGPT</td><td class="border px-8 py-4">27.67%</td><td class="border px-8 py-4">28.31%</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">CVRR-ES</td><td class="border px-8 py-4">cvrr</td><td class="border px-8 py-4">LLaVA-NeXT-Video-7B</td><td class="border px-8 py-4">-</td><td class="border px-8 py-4">44.29%</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">TempCompass</td><td class="border px-8 py-4">tempcompass_caption_matching</td><td class="border px-8 py-4">LLaVA-1.5-13B</td><td class="border px-8 py-4">59.50%</td><td class="border px-8 py-4">59.35%</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">VideoChatGPT</td><td class="border px-8 py-4">videochatgpt_temporal</td><td class="border px-8 py-4">LLaVA-NeXT-Video-7B</td><td class="border px-8 py-4">Score: 2.60 / 5</td><td class="border px-8 py-4">Score: 2.67 / 5</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">NextQA</td><td class="border px-8 py-4">nextqa_oe_test</td><td class="border px-8 py-4">LLaVA-NeXT-Video-7B</td><td class="border px-8 py-4">26.90%</td><td class="border px-8 py-4">26.61%</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">VATEX</td><td class="border px-8 py-4">vatex_test</td><td class="border px-8 py-4">LLaVA-NeXT-Video-7B</td><td class="border px-8 py-4">-</td><td class="border px-8 py-4">CIDEr: 39.28</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">ActivityNetQA</td><td class="border px-8 py-4">activitynetqa</td><td class="border px-8 py-4">LLaVA-NeXT-Video-7B</td><td class="border px-8 py-4">53.5%</td><td class="border px-8 py-4">52.72%</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">VideoDetailCaptions</td><td class="border px-8 py-4">video_dc499</td><td class="border px-8 py-4">LLaVA-NeXT-Video-7B</td><td class="border px-8 py-4">Score: 3.32 / 5</td><td class="border px-8 py-4">Score: 3.50 / 5</td></tr><tr class=hover:bg-gray-50><td class="border px-8 py-4">Video-MME (wo/subs)</td><td class="border px-8 py-4">videomme</td><td class="border px-8 py-4">LLaVA-NeXT-Video-7B</td><td class="border px-8 py-4">-</td><td class="border px-8 py-4">41.98%</td></tr></table><h2 id=more-details-and-feature-updates-with-v020>More Details and Feature Updates with <code>v0.2.0</code></h2><h3 id=improved-pipeline-for-video-evaluations><strong>Improved Pipeline for Video Evaluations</strong></h3><p>Here’s a breakdown of adding video datasets support, especially on how we implement the process from video caching, loading and feed to model to get response.</p><ol><li><p><strong>Download and Load Videos:</strong> Video are being loaded during generation phase. We will host different video datasets on the huggingface and preprocess the video path for you in your huggingface cache folder. It is recommended to set <code>HF_HOME</code> before you use our evaluation suite so that you can manage the download place. After downloading the videos from huggingface hub, we unzip them into a local cache dir, where by default is <code>HF_HOME</code>.</p><ul><li><p>The code specifically demonstrates the logic of how we handle video datasets in lmms-eval.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@retry</span>(stop<span style=color:#f92672>=</span>(stop_after_attempt(<span style=color:#ae81ff>5</span>) <span style=color:#f92672>|</span> stop_after_delay(<span style=color:#ae81ff>60</span>)), wait<span style=color:#f92672>=</span>wait_fixed(<span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>download</span>(self, dataset_kwargs<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># If the dataset is a video dataset,</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Recursively search whether their is a zip and unzip it to the huggingface home</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> dataset_kwargs <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span> <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;video&#34;</span> <span style=color:#f92672>in</span> dataset_kwargs <span style=color:#f92672>and</span> dataset_kwargs[<span style=color:#e6db74>&#34;video&#34;</span>]:
</span></span><span style=display:flex><span>        hf_home <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;HF_HOME&#34;</span>, <span style=color:#e6db74>&#34;~/.cache/huggingface/&#34;</span>)
</span></span><span style=display:flex><span>        cache_dir <span style=color:#f92672>=</span> dataset_kwargs[<span style=color:#e6db74>&#34;cache_dir&#34;</span>]
</span></span><span style=display:flex><span>        cache_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(hf_home, cache_dir)
</span></span><span style=display:flex><span>        accelerator <span style=color:#f92672>=</span> Accelerator()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> accelerator<span style=color:#f92672>.</span>is_main_process:
</span></span><span style=display:flex><span>            cache_path <span style=color:#f92672>=</span> snapshot_download(repo_id<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>DATASET_PATH, repo_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;dataset&#34;</span>)
</span></span><span style=display:flex><span>            zip_files <span style=color:#f92672>=</span> glob(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(cache_path, <span style=color:#e6db74>&#34;**/*.zip&#34;</span>), recursive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(cache_dir) <span style=color:#f92672>and</span> len(zip_files) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> zip_file <span style=color:#f92672>in</span> zip_files:
</span></span><span style=display:flex><span>                    eval_logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Unzipping </span><span style=color:#e6db74>{</span>zip_file<span style=color:#e6db74>}</span><span style=color:#e6db74> to </span><span style=color:#e6db74>{</span>cache_dir<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                    shutil<span style=color:#f92672>.</span>unpack_archive(zip_file, cache_dir)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        accelerator<span style=color:#f92672>.</span>wait_for_everyone()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;builder_script&#34;</span> <span style=color:#f92672>in</span> dataset_kwargs:
</span></span><span style=display:flex><span>            builder_script <span style=color:#f92672>=</span> dataset_kwargs[<span style=color:#e6db74>&#34;builder_script&#34;</span>]
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>DATASET_PATH <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(cache_path, builder_script)
</span></span><span style=display:flex><span>            dataset_kwargs<span style=color:#f92672>.</span>pop(<span style=color:#e6db74>&#34;builder_script&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        dataset_kwargs<span style=color:#f92672>.</span>pop(<span style=color:#e6db74>&#34;cache_dir&#34;</span>)
</span></span><span style=display:flex><span>        dataset_kwargs<span style=color:#f92672>.</span>pop(<span style=color:#e6db74>&#34;video&#34;</span>)
</span></span></code></pre></div></li></ul></li><li><p><strong>Format questions:</strong> For each task, questions are formatted in the <code>&lt;taskname>/utils.py</code> file. We parse each document from the Huggingface dataset, retrieve the questions, and formulate the input with any specified model-specific prompts.</p><ul><li><p>The code specifically demonstrates the logic of how to implement question format.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># This is the place where you format your question</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>perceptiontest_doc_to_text</span>(doc, model_specific_prompt_kwargs<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> model_specific_prompt_kwargs <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        model_specific_prompt_kwargs <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    pre_prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    post_prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;pre_prompt&#34;</span> <span style=color:#f92672>in</span> model_specific_prompt_kwargs:
</span></span><span style=display:flex><span>        pre_prompt <span style=color:#f92672>=</span> model_specific_prompt_kwargs[<span style=color:#e6db74>&#34;pre_prompt&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;post_prompt&#34;</span> <span style=color:#f92672>in</span> model_specific_prompt_kwargs:
</span></span><span style=display:flex><span>        post_prompt <span style=color:#f92672>=</span> model_specific_prompt_kwargs[<span style=color:#e6db74>&#34;post_prompt&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    question <span style=color:#f92672>=</span> doc[<span style=color:#e6db74>&#34;question&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;options&#34;</span> <span style=color:#f92672>in</span> doc:
</span></span><span style=display:flex><span>        index <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> op <span style=color:#f92672>in</span> doc[<span style=color:#e6db74>&#34;options&#34;</span>]:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> index <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                question <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;A. &#34;</span> <span style=color:#f92672>+</span> op
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>elif</span> index <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>                question <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;B. &#34;</span> <span style=color:#f92672>+</span> op
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                question <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;C. &#34;</span> <span style=color:#f92672>+</span> op
</span></span><span style=display:flex><span>            index <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        post_prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Answer with the option&#39;s letter from the given choices directly.&#34;</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;question</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(question)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>pre_prompt<span style=color:#e6db74>}{</span>question<span style=color:#e6db74>}{</span>post_prompt<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div></li></ul></li><li><p><strong>Process results:</strong> After the model generates results, each result is parsed and evaluated based on the corresponding evaluation metric. The choice of metric is based on the dataset’s official implementation on their official project website. We primarily use three types of metrics:</p><p><strong>a. Accuracy:</strong>
For datasets with ground truth answers, we generate a score by comparing the model’s results with the ground truth. This metric is commonly used in multiple-choice QA tasks such as PerceptionTest-val and EgoSchema-subset.</p><p><strong>b. GPT Evaluation:</strong>
For open-ended answers generated by the model, we apply OpenAI GPT API to evaluate the responses. This metric is often used in generation tasks like ActivityNetQA and VideoChatGPT.</p><p><strong>c. Submission:</strong>
If the dataset does not provide ground truth answers and requires submission of inference results to a server for evaluation, we provide a submission file according to the dataset&rsquo;s official template. This metric is used in tasks like EgoSchema, Perception Test.</p></li><li><p><strong>Aggregate results:</strong>
After evaluating each data instance, we aggregate the individual results to generate the overall evaluation metrics. Finally, we provide a summary table that consolidates all the evaluation results, similar to the one in Google’s Gemini report.</p></li><li><p><strong>Grouped Tasks:</strong>
For tasks with multiple subsets, we group all subset tasks together. For example, the VideoChatGPT dataset includes three subsets: generic, temporal, and consistency. By running <code>--task videochatgpt</code>, all three subsets can be evaluated together, eliminating the need to specify each subset individually. We summarize all the grouped task names in Table 1. This pipeline ensures a thorough and standardized evaluation process for video LMMs, facilitating consistent and reliable performance assessment across various tasks and datasets. - This code denotes how we organize the group of tasks together.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>group</span>: <span style=color:#ae81ff>videochatgpt</span>
</span></span><span style=display:flex><span><span style=color:#f92672>task</span>:
</span></span><span style=display:flex><span>- <span style=color:#ae81ff>videochatgpt_gen</span>
</span></span><span style=display:flex><span>- <span style=color:#ae81ff>videochatgpt_temporal</span>
</span></span><span style=display:flex><span>- <span style=color:#ae81ff>videochatgpt_consistency</span>
</span></span></code></pre></div></li></ol><h3 id=improved-overall-evaluation-pipeline><strong>Improved Overall Evaluation Pipeline</strong></h3><ol><li><p>For newly added tasks with different splits and metrics, we have adopted a naming rule in the format <code>{name}_{split}_{metric}</code>. For instance, <code>perceptiontest_val_mcppl</code> refers to the validation split of the PerceptionTest evaluation dataset, using multiple choice perplexity as the evaluation metric.</p></li><li><p>We support <code>llava-next</code> series models with sglang. You can use the following command to launch evaluation with sglang support, that’s much more efficient when running on <code>llava-next-72b/110b</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 -m lmms_eval <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--model<span style=color:#f92672>=</span>llava_sglang <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--model_args<span style=color:#f92672>=</span>pretrained<span style=color:#f92672>=</span>lmms-lab/llava-next-72b,tokenizer<span style=color:#f92672>=</span>lmms-lab/llavanext-qwen-tokenizer,conv_template<span style=color:#f92672>=</span>chatml-llava,tp_size<span style=color:#f92672>=</span>8,parallel<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--tasks<span style=color:#f92672>=</span>mme <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--log_samples <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--log_samples_suffix<span style=color:#f92672>=</span>llava_qwen <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--output_path<span style=color:#f92672>=</span>./logs/ <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--verbosity<span style=color:#f92672>=</span>INFO
</span></span></code></pre></div></li><li><p>We add a <code>force_download</code> mode to robustly handle the case that videos are not fully cached in local folder. You could add the args to task yaml file as the following commands. To support the evaluations that in machines that do not have access to internet, we add the <code>local_files_only</code> to support this feature.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>dataset_path</span>: <span style=color:#ae81ff>lmms-lab/ActivityNetQA</span>
</span></span><span style=display:flex><span><span style=color:#f92672>dataset_kwargs</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>token</span>: <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>video</span>: <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>force_download</span>: <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>local_files_only</span>: <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>cache_dir</span>: <span style=color:#ae81ff>activitynetqa</span>
</span></span><span style=display:flex><span><span style=color:#f92672>model_specific_prompt_kwargs</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>default</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>pre_prompt</span>: <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>post_prompt</span>: <span style=color:#e6db74>&#34; Answer the question using a single word or phrase.&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>version</span>: <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>gpt_eval_model_name</span>: <span style=color:#ae81ff>gpt-3.5-turbo-0613</span>
</span></span></code></pre></div></li><li><p>We found that sometimes the dataset downloading process will throw <code>Retry</code> or <code>HTTP Timedout</code> errors. To prevent this, we recommend disabling <code>hf_transfer</code> mechanism by setting this in your environment <code>export HF_HUB_ENABLE_HF_TRANSFER="0"</code></p></li><li><p>mmmu_group_img_val</p><p>We aligned the results of LLaVA-NeXT 34B with previously reported values. In our previous evaluation, for the questions with multiple images, we concatenated them into one. When tested separately (<code>mmmu_val</code>), the score was 46.7, and after we do the concatenation operation (in lmms-eval, you could switch to use <code>tasks=mmmu_group_img_val</code>), the score was 50.1 for LLaVA-NeXT 34B.</p><details><summary>Example Images and QA Pairs</summary><p align=center><a href=https://postimg.cc/2brN0TMk><img src=https://i.postimg.cc/JnB742Qk/mmmu-group.png alt=mmmu-group.png></a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>A collimated beam containing two different frequencies of light travels through vacuum and is incident on a piece of glass. Which of the schematics below depicts the phenomenon of dispersion within the glass in a qualitative correct manner? Select <span style=color:#f92672>(</span>e<span style=color:#f92672>)</span> <span style=color:#66d9ef>if</span> none of the options are qualitatively correct.
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>A<span style=color:#f92672>)</span> &lt;image 1&gt;
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>B<span style=color:#f92672>)</span> &lt;image 2&gt;
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>C<span style=color:#f92672>)</span> &lt;image 3&gt;
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>D<span style=color:#f92672>)</span> &lt;image 4&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Answer with the option<span style=color:#960050;background-color:#1e0010>&#39;</span>s letter from the given choices directly.
</span></span></code></pre></div></details><br></li><li><p><strong>Predict Only Mode - Only Inference, No Evaluation</strong></p><p>In some cases, you may want to obtain only the inference results, without triggering the evaluation process. For this purpose, we have integrate the <strong><code>predict_only</code></strong> mode from the original <code>lmms-eval</code>. This feature allows you to obtain model inference results without performing further evaluation. <strong>It is particularly useful when you do not need to evaluate your model results, for instance, if the dataset requires ChatGPT-based evaluation but you do not want to use the OpenAI API.</strong></p><p>To use the <strong><code>predict_only</code></strong> mode, add the <strong><code>--predict_only</code></strong> flag to your command. This will override the original evaluation process with a bypass function after obtaining the model inference results and simply save the results as logs.</p></li><li><p><strong>From-Logs Mode - Evaluation Based on Log Files from <code>predict_only</code> mode</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>accelerate launch -m lmms_eval <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--model<span style=color:#f92672>=</span>from_log <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--tasks<span style=color:#f92672>=</span>&lt;taskname&gt; <span style=color:#ae81ff>\ </span>
</span></span><span style=display:flex><span>--batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--log_samples <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--output_path<span style=color:#f92672>=</span>./logs/ <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--model_args<span style=color:#f92672>=</span>logs<span style=color:#f92672>=</span>&lt;path_to_log_directory&gt;,model_name<span style=color:#f92672>=</span>&lt;model_name&gt;,model_args <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--verbosity<span style=color:#f92672>=</span>DEBUG
</span></span></code></pre></div><p>In some cases, you may want to evaluate model performance using pre-existing inference results. For this purpose, we have designed the <strong><code>from_log</code></strong> mode. This feature allows you to evaluate model performance directly from inference results recorded in the <code>logs/</code> directory. This mode saves time and enhances portability, consistency, and reproducibility. <strong>It is particularly useful when you already have model inference results and wish to avoid running the inference process again.</strong></p><p>Currently, we only support inference results stored in the <code>logs/</code> directory, which is the log file generated from our pipeline. Hence currently, the file template of these results is pre-defined. If you need to evaluate the inference result generated by yourself, you may have to convert your file into the same template as those under <code>logs/</code>.</p><p>To use the <strong><code>from_log</code></strong> mode for performance evaluation based on existing log files, you can run the following command. You can specify the <code>task_name</code>, <code>path_to_log_directory</code>, <code>model_name</code>and <code>model_args</code>(if specified). Our framework will traverse through all the log files within the specified directory and find the most recent log file for evaluation.</p></li><li><p><strong>Combined Use of Two Modes</strong></p><p>You can use the two modes together: using the <strong><code>predict_only</code></strong> mode to obtain model inference results, and using the <strong><code>from_log</code></strong> mode to evaluate the generated inference results. This enhances the overall consistency and reproducibility of our framework.</p><p>You can use the following command to run the two modes together:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># Open predict_only mode to inference</span>
</span></span><span style=display:flex><span>accelerate launch --num_processes <span style=color:#ae81ff>8</span> --main_process_port <span style=color:#ae81ff>12345</span> -m lmms_eval <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --model &lt;model_name&gt; <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --tasks $TASK <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --batch_size <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --log_samples <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --log_samples_suffix &lt;model_name&gt; <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --output_path ./logs/ <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --predict_only
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Open from_log mode to evaluate</span>
</span></span><span style=display:flex><span>accelerate launch --num_processes <span style=color:#ae81ff>8</span> --main_process_port <span style=color:#ae81ff>12345</span> -m lmms_eval <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --model from_log <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --model_args model_name<span style=color:#f92672>=</span>&lt;model_name&gt;<span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --tasks $TASKS <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --batch_size <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --log_samples <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --log_samples_suffix &lt;model_name&gt; <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --output_path ./logs/
</span></span></code></pre></div></li></ol><h3 id=newly-supported-video-tasks><strong>Newly Supported Video Tasks</strong></h3><ol><li><a href=https://github.com/MILVLG/activitynet-qa>ActivityNet-QA</a></li><li><a href=https://github.com/egoschema/EgoSchema/>EgoSchema</a></li><li><a href=http://youcook2.eecs.umich.edu/>YouCook2</a></li><li><a href=https://eric-xw.github.io/vatex-website/index.html>VATEX</a></li><li><a href=https://eric-xw.github.io/vatex-website/index.html>VATEX-ZH</a></li><li><a href=https://github.com/mbzuai-oryx/Video-ChatGPT/>VideoChatGPT</a></li><li><a href=https://github.com/EvolvingLMMs-Lab/lmms-eval-internal/blob/internal_main_dev/lmms_eval/tasks/video_detail_description/README.md>VideoDetailCaptions</a></li><li><a href=https://github.com/doc-doc/NExT-QA>NextQA</a></li><li><a href=https://github.com/mbzuai-oryx/CVRR-Evaluation-Suite/>CVRR-ES</a></li><li><a href=https://github.com/google-deepmind/perception_test>Perception Test</a></li><li><a href=https://github.com/llyx97/TempCompass>TempCompass</a></li><li><a href=https://github.com/BradyFU/Video-MME>Video-MME</a></li></ol><h3 id=newly-supported-video-models><strong>Newly Supported Video Models</strong></h3><p>We have supported more video models that can be used in LMMs-Eval. We now support evaluating video datasets using a one line command.</p><ol><li><a href=https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944>LLaVA-NeXT-Video</a></li><li><a href=https://github.com/PKU-YuanGroup/Video-LLaVA>Video-LLaVA</a></li><li><a href=https://github.com/dvlab-research/LLaMA-VID>LLaMA-VID</a></li><li><a href=https://github.com/mbzuai-oryx/Video-ChatGPT>Video-ChatGPT</a></li><li><a href=https://github.com/X-PLUG/mPLUG-Owl>MPLUG-OWL</a></li></ol><h3 id=community-support><strong>Community Support</strong></h3><p>During this period, we received the following Pull Requests (PRs):</p><blockquote><p>Details are in <a href=https://github.com/EvolvingLMMs-Lab/lmms-eval/releases/tag/untagged-9057ff0e9a72d5a5846f>lmms-eval/v0.2.0 release notes</a></p></blockquote><p><strong>Datasets:</strong></p><ul><li>VCR: Vision Caption Restoration (officially from the authors, MILA)</li><li>ConBench (officially from the authors, PKU/Bytedance)</li><li>MathVerse (officially from the authors, CUHK)</li><li>MM-UPD (officially from the authors, University of Tokyo)</li><li>WebSRC (from Hunter Heiden)</li><li>ScreeSpot (from Hunter Heiden)</li><li>RealworldQA (from Fanyi Pu, NTU)</li><li>Multi-lingual LLaVA-W (from Gagan Bhatia, UBC)</li></ul><p><strong>Models:</strong></p><ul><li>LLaVA-HF (officially from Huggingface)</li><li>Idefics-2 (from the lmms-lab team)</li><li>microsoft/Phi-3-Vision (officially from the authors, Microsoft)</li><li>LLaVA-SGlang (from the lmms-lab team)</li></ul></div></article><script defer src=https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js></script><script src=/js/darkmode.js defer></script><footer class=bg-gray-900><div class="max-w-md px-4 py-12 mx-auto overflow-hidden sm:max-w-3xl sm:px-6 lg:max-w-7xl lg:px-8"><nav class="flex flex-wrap justify-center -mx-5 -my-2" aria-label=Footer><div class="px-5 py-2"><a href=/team/ class="text-base text-gray-400 hover:text-gray-300">Team</a></div><div class="px-5 py-2"><a href=/news/ class="text-base text-gray-400 hover:text-gray-300">News</a></div><div class="px-5 py-2"><a href=/categories/blog/ class="text-base text-gray-400 hover:text-gray-300">Blog</a></div></nav><div class="flex justify-center mt-8 space-x-6"><a href=# class="text-gray-400 hover:text-gray-300"><span class=sr-only>Twitter</span>
<svg class="w-6 h-6" fill="currentcolor" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.29 20.251c7.547.0 11.675-6.253 11.675-11.675.0-.178.0-.355-.012-.53A8.348 8.348.0 0022 5.92a8.19 8.19.0 01-2.357.646 4.118 4.118.0 001.804-2.27 8.224 8.224.0 01-2.605.996 4.107 4.107.0 00-6.993 3.743A11.65 11.65.0 013.392 4.748a4.106 4.106.0 001.27 5.477A4.072 4.072.0 012.8 9.713v.052a4.105 4.105.0 003.292 4.022 4.095 4.095.0 01-1.853.07 4.108 4.108.0 003.834 2.85A8.233 8.233.0 012 18.407a11.616 11.616.0 006.29 1.84"/></svg>
</a><a href=# class="text-gray-400 hover:text-gray-300"><span class=sr-only>GitHub</span><svg class="w-6 h-6" fill="currentcolor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483.0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951.0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65.0.0.84-.27 2.75 1.026A9.564 9.564.0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688.0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855.0 1.338-.012 2.419-.012 2.747.0.268.18.58.688.482A10.019 10.019.0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"/></svg></a></div><p class="mt-8 text-base text-center text-gray-400">&copy; 2024
LMMs-Lab. All rights
reserved.</p><p class="mt-2 text-base text-center text-gray-400">Made with &#x2764;&#xfe0f; by <a href=https://nusserstudios.com class="hover:underline hover:text-primary-600"><span class="font-black uppercase">Nusser</span> <span class="font-light uppercase">Studios.</span></a></p></div></footer></body></html>