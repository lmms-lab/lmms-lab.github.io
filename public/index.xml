<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LMMs-Lab</title>
    <link>//localhost:1313/</link>
    <description>Recent content on LMMs-Lab</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Jun 2024 03:23:23 +0800</lastBuildDate>
    <atom:link href="//localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Embracing Video Evaluations with LMMs-Eval</title>
      <link>//localhost:1313/posts/lmms-eval-0.2/</link>
      <pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/lmms-eval-0.2/</guid>
      <description>We introduce a video evaluation feature to lmms-eval, supporting video model evaluations with over most popular datasets.</description>
    </item>
    <item>
      <title>News</title>
      <link>//localhost:1313/news/</link>
      <pubDate>Thu, 06 Jun 2024 11:10:36 +0800</pubDate>
      <guid>//localhost:1313/news/</guid>
      <description>News [2024-06] ðŸŽ¬ðŸŽ¬ The lmms-eval/v0.2 has been upgraded to support video evaluations for video models like LLaVA-NeXT Video and Gemini 1.5 Pro across tasks such as EgoSchema, PerceptionTest, VideoMME, and more.&#xA;GitHub | Blog&#xA;[2024-05] ðŸš€ðŸš€ We release the LLaVA-NeXT Video, a video model with state-of-the-art performance and reaching to Google&amp;rsquo;s Gemini level performance on diverse video understanding tasks.&#xA;GitHub | Blog&#xA;[2024-05] ðŸš€ðŸš€ We release the LLaVA-NeXT with state-of-the-art and near GPT-4V performance at multiple multimodal benchmarks.</description>
    </item>
    <item>
      <title>Team Information</title>
      <link>//localhost:1313/team/</link>
      <pubDate>Thu, 06 Jun 2024 11:10:36 +0800</pubDate>
      <guid>//localhost:1313/team/</guid>
      <description>What is LMMs-Lab? Large-scale Multimodal Models Research Lab (LMMs-Lab) is a team of students, researchers, faculty who are passionate about the future of multimodal models. We aim to study the multimodal models, including the training of state-of-the-art level models, the holistic evaluations of multimodal models, and more.&#xA;We are always looking for new members to join us, so if you are interested, please contact us.&#xA;Members Student Team Bo Li, PhD Student, NTU, Singapore Yuanhan Zhang, PhD Student, NTU, Singapore Peiyuan Zhang, Research Assistant, NTU -&amp;gt; PhD Student, UCSD Kaichen Zhang, Undergraduate Student, NTU, Singapore Fanyi Pu, Undergraduate Student, NTU, Singapore Kairui Hu, Undergraduate Student, NTU, Singapore Jingkang Yang, PhD Student, NTU, Singapore Shuai Liu, MS Student, NTU, Singapore Faculty Team Chunyuan Li, ByteDance Ziwei Liu, Assistant Professor, NTU, Singapore Contacts Twitter: @LMMs_Lab Discord: LMMs-Lab </description>
    </item>
    <item>
      <title>Accelerating the Development of Large Multimodal Models with LMMs-Eval</title>
      <link>//localhost:1313/posts/lmms-eval-0.1/</link>
      <pubDate>Thu, 07 Mar 2024 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/lmms-eval-0.1/</guid>
      <description>One command evaluation API for fast and thorough evaluation of LMMs, providing multi-faceted insights on model performance with over 40 datasets.</description>
    </item>
    <item>
      <title>News Post 1</title>
      <link>//localhost:1313/posts/news-post-1/</link>
      <pubDate>Wed, 18 May 2022 11:10:36 +0800</pubDate>
      <guid>//localhost:1313/posts/news-post-1/</guid>
      <description>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed cursus, odio nec venenatis lacinia, lacus lectus varius nisi, in tristique mi purus ut libero.</description>
    </item>
    <item>
      <title>News Post 2</title>
      <link>//localhost:1313/posts/news-post-2/</link>
      <pubDate>Mon, 18 Apr 2022 11:10:36 +0800</pubDate>
      <guid>//localhost:1313/posts/news-post-2/</guid>
      <description>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed cursus, odio nec venenatis lacinia, lacus lectus varius nisi, in tristique mi purus ut libero.</description>
    </item>
    <item>
      <title>News Post 4</title>
      <link>//localhost:1313/posts/news-post-4/</link>
      <pubDate>Fri, 18 Mar 2022 11:10:36 +0800</pubDate>
      <guid>//localhost:1313/posts/news-post-4/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed cursus, odio nec venenatis lacinia, lacus lectus varius nisi, in tristique mi purus ut libero.&lt;/p&gt;</description>
    </item>
    <item>
      <title>News Post 5</title>
      <link>//localhost:1313/posts/news-post-5/</link>
      <pubDate>Fri, 18 Mar 2022 11:10:36 +0800</pubDate>
      <guid>//localhost:1313/posts/news-post-5/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed cursus, odio nec venenatis lacinia, lacus lectus varius nisi, in tristique mi purus ut libero.&lt;/p&gt;</description>
    </item>
    <item>
      <title>News Post 6</title>
      <link>//localhost:1313/posts/news-post-6/</link>
      <pubDate>Fri, 18 Mar 2022 11:10:36 +0800</pubDate>
      <guid>//localhost:1313/posts/news-post-6/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed cursus, odio nec venenatis lacinia, lacus lectus varius nisi, in tristique mi purus ut libero.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Contact</title>
      <link>//localhost:1313/contact/</link>
      <pubDate>Sat, 18 Dec 2021 03:10:36 +0000</pubDate>
      <guid>//localhost:1313/contact/</guid>
      <description> If you wish to join our community, feel free to contact us.&#xA;Your Email: Subject: Your message: Send Message </description>
    </item>
    <item>
      <title>News Post 3</title>
      <link>//localhost:1313/posts/news-post-3/</link>
      <pubDate>Sat, 18 Dec 2021 11:10:36 +0800</pubDate>
      <guid>//localhost:1313/posts/news-post-3/</guid>
      <description>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed cursus, odio nec venenatis lacinia, lacus lectus varius nisi, in tristique mi purus ut libero.</description>
    </item>
  </channel>
</rss>
