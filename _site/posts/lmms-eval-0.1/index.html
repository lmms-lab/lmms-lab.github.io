<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="apple-touch-icon" sizes="180x180" href="//localhost:1313/favicon/fav.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/favicon/fav.png">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/favicon/fav.png">
<link rel="manifest" href="//localhost:1313/favicon/site.webmanifest">
<link rel="mask-icon" href="//localhost:1313/favicon/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<title itemprop="name"> Accelerating the Development of Large Multimodal Models with LMMs-Eval | LMMs-Lab 
</title>
<meta name="description" content="" />
<meta property="og:title" content="Accelerating the Development of Large Multimodal Models with LMMs-Eval | LMMs-Lab" />
<meta name="twitter:title" content="Accelerating the Development of Large Multimodal Models with LMMs-Eval | LMMs-Lab" />
<meta itemprop="name" content="Accelerating the Development of Large Multimodal Models with LMMs-Eval | LMMs-Lab" />
<meta name="application-name" content="Accelerating the Development of Large Multimodal Models with LMMs-Eval | LMMs-Lab" />
<meta property="og:description"
      content="" />
<meta property="og:site_name" content="LMMs-Lab" />
<meta property="og:url" content="//localhost:1313/posts/lmms-eval-0.1/" />
<meta property="og:locale" content="en">
<meta property="og:image" content="/../assets/images/pages/lmms-eval.png" />
<meta property="og:image:secure_url" content="//localhost:1313/assets/images/pages/lmms-eval.png" />
<meta property="og:type" content="article" />

<script>
    
    if (localStorage.getItem('color-theme') === 'dark' || (!('color-theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
        document.documentElement.classList.add('dark');
    } else {
        document.documentElement.classList.remove('dark')
    }
</script>


<link rel="stylesheet" href="//localhost:1313/css/style.css" />

</head>

<body class="bg-zinc-100 dark:bg-gray-800">
    <div class="top-0 z-50 w-full text-gray-200 bg-gray-900 border-2 border-gray-900 md:sticky border-b-stone-200/10">
    <div x-data="{ open: false }"
         class="flex flex-col max-w-full px-4 mx-auto md:items-center md:justify-between md:flex-row md:px-6 lg:px-8">
        <div class="flex flex-row items-center justify-between p-4">
            <a href="//localhost:1313/" class="flex text-gray-100 transition duration-1000 ease-in-out group">
                <img src="//localhost:1313/images/fav.png"
                     class="transition-opacity h-9 w-9 group-hover:opacity-50 group-focus:opacity-70"
                     alt="LMMs-Lab Logo" />
                <div
                     class="mt-1 ml-3 text-xl font-black tracking-tight text-gray-100 uppercase transition-colors group-hover:text-gray-400/60">
                    LMMs-Lab</div>
            </a>
            <button class="rounded-lg md:hidden focus:outline-none focus:shadow-outline" @click="open = !open" role="navigation" aria-expanded="false" aria-label="Main" aria-controls="menuItems">
                <svg fill="currentColor" viewBox="0 0 20 20" class="w-6 h-6">
                    <path x-show="!open" fill-rule="evenodd"
                          d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM9 15a1 1 0 011-1h6a1 1 0 110 2h-6a1 1 0 01-1-1z"
                          clip-rule="evenodd"></path>
                    <path x-show="open" fill-rule="evenodd"
                          d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z"
                          clip-rule="evenodd"></path>
                </svg>
            </button>
        </div>
        <nav :class="{'flex': open, 'hidden': !open}"
             class="flex-col flex-grow hidden pb-4 md:pb-0 md:flex md:justify-end md:flex-row">
            
            
            <a class="px-4 py-2 mt-2 text-sm font-semibold rounded-lg md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline"
                href="//localhost:1313/team/" title="Team">
                Team
            </a>
            
            
            
            <a class="px-4 py-2 mt-2 text-sm font-semibold rounded-lg md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline"
                href="//localhost:1313/categories/blog/" title="Blogs">
                Blogs
            </a>
            
            
            
            <a class="px-4 py-2 mt-2 text-sm font-semibold rounded-lg md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline"
                href="//localhost:1313/news/" title="News">
                News
            </a>
            
            
            
            <div @click.away="open = false" class="relative" x-data="{ open: false }">
                <button @click="open = !open"
                        class="flex flex-row items-center w-full px-4 py-2 mt-2 text-sm font-semibold text-left bg-transparent rounded-lg dark-mode:focus:text-white dark-mode:hover:text-white dark-mode:focus:bg-gray-600 dark-mode:hover:bg-gray-600 md:w-auto md:inline md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-600 focus:outline-none focus:shadow-outline">
                    <span>More</span>
                    <svg fill="currentColor" viewBox="0 0 20 20" :class="{'rotate-180': open, 'rotate-0': !open}"
                         class="inline w-4 h-4 mt-1 ml-1 transition-transform duration-200 transform md:-mt-1">
                        <path fill-rule="evenodd"
                              d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z"
                              clip-rule="evenodd"></path>
                    </svg>
                </button>
                <div x-show="open" x-transition:enter="transition ease-out duration-100"
                     x-transition:enter-start="transform opacity-0 scale-95"
                     x-transition:enter-end="transform opacity-100 scale-100"
                     x-transition:leave="transition ease-in duration-75"
                     x-transition:leave-start="transform opacity-100 scale-100"
                     x-transition:leave-end="transform opacity-0 scale-95"
                     class="absolute right-0 z-30 w-full mt-2 origin-top-right md:max-w-sm md:w-screen">
                    <div class="px-2 pt-2 pb-4 bg-white rounded-md shadow-lg text-zinc-900">
                        <div class="grid grid-cols-1 gap-4">
                            
                            <a class="flex items-start p-2 bg-transparent rounded-lg group row hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline"
                               href="https://github.com/EvolvingLMMs-Lab">
                                <div class="p-3 text-white bg-primary-600 rounded-lg group-hover:bg-gray-900">
                                    
                                    <svg fill="none" stroke="currentColor" stroke-linecap="round"
                                         stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24"
                                         class="w-4 h-4 md:h-6 md:w-6">
                                        <path
                                              d="M5 3v4M3 5h4M6 17v4m-2-2h4m5-16l2.286 6.857L21 12l-5.714 2.143L13 21l-2.286-6.857L5 12l5.714-2.143L13 3z">
                                        </path>
                                    </svg>
                                    
                                    
                                </div>
                                <div class="ml-3">
                                    <p class="font-semibold">Github</p>
                                    <p class="text-sm">See what we're grinding on</p>
                                </div>
                            </a>
                            
                            <a class="flex items-start p-2 bg-transparent rounded-lg group row hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline"
                               href="https://huggingface.co/lmms-lab">
                                <div class="p-3 text-white bg-primary-600 rounded-lg group-hover:bg-gray-900">
                                    
                                    <svg fill="none" stroke="currentColor" stroke-linecap="round"
                                         stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24"
                                         class="w-4 h-4 md:h-6 md:w-6">
                                        <path
                                              d="M8 12h.01M12 12h.01M16 12h.01M21 12c0 4.418-4.03 8-9 8a9.863 9.863 0 01-4.255-.949L3 20l1.395-3.72C3.512 15.042 3 13.574 3 12c0-4.418 4.03-8 9-8s9 3.582 9 8z">
                                        </path>
                                    </svg>
                                    
                                    
                                </div>
                                <div class="ml-3">
                                    <p class="font-semibold">Huggingface</p>
                                    <p class="text-sm">See what we're opensourcing</p>
                                </div>
                            </a>
                            
                            <a class="flex items-start p-2 bg-transparent rounded-lg group row hover:text-white focus:text-white hover:bg-primary-600 focus:bg-primary-700 focus:outline-none focus:shadow-outline"
                               href="https://github.com/LLaVA-VL/LLaVA-NeXT">
                                <div class="p-3 text-white bg-primary-600 rounded-lg group-hover:bg-gray-900">
                                    
                                    <svg fill="none" stroke="currentColor" stroke-linecap="round"
                                         stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24"
                                         class="w-4 h-4 md:h-6 md:w-6">
                                        <path d="M11 3.055A9.001 9.001 0 1020.945 13H11V3.055z">
                                        </path>
                                        <path d="M20.488 9H15V3.512A9.025 9.025 0 0120.488 9z">
                                        </path>
                                    </svg>
                                    
                                    
                                </div>
                                <div class="ml-3">
                                    <p class="font-semibold">Related</p>
                                    <p class="text-sm">Take a look at our related projects</p>
                                </div>
                            </a>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            
        </nav>
    </div>
</div>
    
<article>
  <header class="mb-4 bg-primary-600">
    <span class="py-96">
      <h1 class="py-16 text-5xl font-black text-center text-white capitalize">
        Accelerating the Development of Large Multimodal Models with LMMs-Eval
      </h1>
    </span>
  </header>
  <div class="max-w-4xl mx-auto mt-8 mb-2">
    <div class="px-6">
      
      
      
      
      
      
      <img src="//localhost:1313/../assets/images/pages/lmms-eval_hub88987c4b4178ec17ff4ec65c0a266c4_2550007_1500x0_resize_q80_h2_box_3.webp" srcset=", /../assets/images/pages/lmms-eval_hub88987c4b4178ec17ff4ec65c0a266c4_2550007_400x0_resize_q80_h2_box_3.webp 400w, /../assets/images/pages/lmms-eval_hub88987c4b4178ec17ff4ec65c0a266c4_2550007_550x0_resize_q80_h2_box_3.webp 550w, /../assets/images/pages/lmms-eval_hub88987c4b4178ec17ff4ec65c0a266c4_2550007_900x0_resize_q80_h2_box_3.webp 768w, /../assets/images/pages/lmms-eval_hub88987c4b4178ec17ff4ec65c0a266c4_2550007_1500x0_resize_q80_h2_box_3.webp 1100w"
           class="object-fill overflow-hidden rounded-lg shadow-lg ring-4 ring-zinc-300/40 dark:ring-gray-900/40 shadow-neutral-100/20 dark:shadow-neutral-800/40"
           width="100%" alt="" />
      
      
    </div>
  </div>
  
  <div class="max-w-4xl px-6 pt-6 pb-16 mx-auto prose dark:prose-invert dark:text-white">
    <h2 id="introduction">Introduction</h2>
<p>In today&rsquo;s world, we&rsquo;re on a thrilling quest for Artificial General Intelligence (AGI), driven by a passion that reminds us of the excitement surrounding the 1960s moon landing. At the heart of this adventure are the incredible large language models (LLMs) and large multimodal models (LMMs). These models are like brilliant minds that can understand, learn, and interact with a vast array of human tasks, marking a significant leap toward our goal.</p>
<p>To truly understand how capable these models are, we&rsquo;ve started to create and use a wide variety of evaluation benchmarks. These benchmarks help us map out a detailed chart of abilities, showing us how close we are to achieving true AGI. However, this journey is not without its challenges. The sheer number of benchmarks and datasets we need to look at is overwhelming. They&rsquo;re all over the place - tucked away in someone&rsquo;s Google Drive, scattered across Dropbox, and hidden in the corners of various school and research lab websites. It&rsquo;s like embarking on a treasure hunt where the maps are spread far and wide.</p>
<p>In the field of language models, there has been a valuable precedent set by the work of <code>lm-evaluation-harness</code> <d-cite key="eval-harness"></d-cite>. They offer integrated data and model interfaces, enabling rapid evaluation of language models and serving as the backend support framework for the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">open-llm-leaderboard</a>, and has gradually become the underlying ecosystem of the era of foundation models.</p>
<p>However, the evaluation of multi-modality models is still in its infancy, and there is no unified evaluation framework that can be used to evaluate multi-modality models across a wide range of datasets. To address this challenge, we introduce <strong>lmms-eval</strong><d-cite key="lmms_eval2024"></d-cite>, an evaluation framework meticulously crafted for consistent and efficient evaluation of Large Multimoal Models (LMMs).</p>
<p>We humbly obsorbed the exquisite and efficient design of <a href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a>. Building upon its foundation, we implemented our <code>lmms-eval</code> framework with performance optimizations specifically for LMMs.</p>
<h2 id="necessity-of-lmms-eval">Necessity of lmms-eval</h2>
<p>We believe our effort could provide an efficient interface for the detailed comparison of publicly available models to discern their strengths and weaknesses. Itâ€™s also useful for research institutions and production-oriented companies to accelerate the development of Large Multimoal Models. With the <code>lmms-eval</code>, we have significantly accelerated the lifecycle of model iteration. Inside the LLaVA team, the utilization of <code>lmms-eval</code> largely improves the efficiency of the model development cycle, as we are able to evaluate weekly trained hundreds of checkpoints on 20-30 datasets, identifying the strengths and weaknesses, and then make targeted improvements.</p>
<hr>
<h2 id="important-features">Important Features</h2>
<p><img src="https://lmms-lab.github.io/blog-images/lmms-eval-1.0/teaser.webp" alt=""></p>
<!-- {% include figure.liquid loading="eager" path="assets/img/teaser.png" class="img-fluid rounded z-depth-1" zoomable=true %} -->
<!-- <p align="center" width="100%">
<img src="assets/img/teaser.png"  width="100%" height="100%">
</p> -->
<p>For more usage guidance, please visit our <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main">GitHub repo</a>.</p>
<h3 id="one-command-evaluation-with-detailed-logs-and-samples">One-command evaluation, with detailed logs and samples.</h3>
<p>You can evaluate the models on multiple datasets with a single command. No model/data preparation is needed, just one command line, few minutes, and get the results. Not just a result number, but also the detailed logs and samples, including the model args, input question, model response, and ground truth answer.</p>
<h3 id="accelerator-support-and-tasks-grouping">Accelerator support and Tasks grouping.</h3>
<p>We support the usage of <code>accelerate</code> to wrap the model for distributed evaluation, supporting multi-gpu and tensor parallelism. With <strong>Task Grouping</strong>, all instances from all tasks are grouped and evaluated in parallel, which significantly improves the throughput of the evaluation.</p>
<p>Below are the total runtime on different datasets using 4 x A100 40G.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Dataset (#num)</th>
<th style="text-align:left">LLaVA-v1.5-7b</th>
<th style="text-align:left">LLaVA-v1.5-13b</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">mme (2374)</td>
<td style="text-align:left">2 mins 43 seconds</td>
<td style="text-align:left">3 mins 27 seconds</td>
</tr>
<tr>
<td style="text-align:left">gqa (12578)</td>
<td style="text-align:left">10 mins 43 seconds</td>
<td style="text-align:left">14 mins 23 seconds</td>
</tr>
<tr>
<td style="text-align:left">scienceqa_img (2017)</td>
<td style="text-align:left">1 mins 58 seconds</td>
<td style="text-align:left">2 mins 52 seconds</td>
</tr>
<tr>
<td style="text-align:left">ai2d (3088)</td>
<td style="text-align:left">3 mins 17 seconds</td>
<td style="text-align:left">4 mins 12 seconds</td>
</tr>
<tr>
<td style="text-align:left">coco2017_cap_val (5000)</td>
<td style="text-align:left">14 mins 13 seconds</td>
<td style="text-align:left">19 mins 58 seconds</td>
</tr>
</tbody>
</table>
<h3 id="all-in-one-hf-dataset-hubs">All-In-One HF dataset hubs.</h3>
<p>We are hosting more than 40 (and increasing) datasets on <a href="https://huggingface.co/lmms-lab">huggingface/lmms-lab</a>, we carefully converted these datasets from original sources and included all variants, versions and splits. Now they can be directly accessed without any burden of data preprocessing. They also serve for the purpose of visualizing the data and grasping the sense of evaluation tasks distribution.</p>
<!-- {% include figure.liquid loading="eager" path="assets/img/ai2d.png" class="img-fluid rounded z-depth-1" zoomable=true %} -->
<p><img src="https://lmms-lab.github.io/blog-images/lmms-eval-1.0/ai2d.webp" alt=""></p>
<h3 id="detailed-logging-utilites">Detailed Logging Utilites</h3>
<p>We provide detailed logging utilities to help you understand the evaluation process and results. The logs include the model args, generation parameters, input question, model response, and ground truth answer. You can also record every details and visualize them inside runs on Weights &amp; Biases.</p>
<p><img src="https://lmms-lab.github.io/blog-images/lmms-eval-1.0/wandb_table.webp" alt=""></p>
<!-- <p align="center" width="100%">
<img src="https://i.postimg.cc/W1c1vBDJ/Wechat-IMG1993.png"  width="100%" height="80%">
</p> -->
<h2 id="model-results">Model Results</h2>
<p>As demonstrated by the extensive table below, we aim to provide detailed information for readers to understand the datasets included in lmms-eval and some specific details about these datasets (we remain grateful for any corrections readers may have during our evaluation process).</p>
<p>We provide a Google Sheet for the detailed results of the LLaVA series models on different datasets. You can access the sheet <a href="https://docs.google.com/spreadsheets/d/1a5ImfdKATDI8T7Cwh6eH-bEsnQFzanFraFUgcS9KHWc/edit?usp=sharing">here</a>. It&rsquo;s a live sheet, and we are updating it with new results.</p>
<!-- {% include figure.liquid loading="eager" path="assets/img/sheet_table.png" class="img-fluid rounded z-depth-1" zoomable=true %} -->
<p><img src="https://lmms-lab.github.io/blog-images/lmms-eval-1.0/sheet_table.webp" alt=""></p>
<!-- <p align="center" width="100%">
<img src="https://i.postimg.cc/jdw497NS/WX20240307-162526-2x.png"  width="100%" height="80%">
</p> -->
<p>We also provide the raw data exported from Weights &amp; Biases for the detailed results of the LLaVA series models on different datasets. You can access the raw data <a href="https://docs.google.com/spreadsheets/d/1AvaEmuG4csSmXaHjgu4ei1KBMmNNW8wflOD_kkTDdv8/edit?usp=sharing">here</a>.</p>
<h2 id="supported-modelsdatasets">Supported Models/Datasets</h2>
<p>Different models perform best at specific prompt strategies and require models developers skilled knowledge to implement. We prefer not to hastily integrate a model without a thorough understanding to it. Our focus is on more tasks, and we encourage model developers to incorporate our framework into their development process and, when appropriate, integrate their model implementations into our framework through PRs (Pull Requests). Within this strategy, we support a wide range of datasets and selective model series, including:</p>
<h3 id="supported-models">Supported models</h3>
<ul>
<li>GPT4V (API, only generation-based evaluation)</li>
<li>LLaVA series (ppl-based, generation-based)</li>
<li>Qwen-VL series (ppl-based, generation-based)</li>
<li>Fuyu series (ppl-based, generation-based)</li>
<li>InstructBLIP series (generation-based)</li>
<li>MiniCPM-V series (ppl-based, generation-based)</li>
</ul>
<h3 id="supported-datasets">Supported datasets</h3>
<p>Names inside <code>()</code> indicate the actual task name referred in the config file.</p>
<ul>
<li>AI2D <code>(ai2d)</code></li>
<li>ChartQA <code>(chartqa)</code></li>
<li>CMMMU <code>(cmmmu)</code>
<ul>
<li>CMMMU Validation <code>(cmmmu_val)</code></li>
<li>CMMMU Test <code>(cmmmu_test)</code></li>
</ul>
</li>
<li>COCO Caption <code>(coco_cap)</code>
<ul>
<li>COCO 2014 Caption <code>(coco2014_cap)</code>
<ul>
<li>COCO 2014 Caption Validation <code>(coco2014_cap_val)</code></li>
<li>COCO 2014 Caption Test <code>(coco2014_cap_test)</code></li>
</ul>
</li>
<li>COCO 2017 Caption <code>(coco2017_cap)</code>
<ul>
<li>COCO 2017 Caption MiniVal <code>(coco2017_cap_val)</code></li>
<li>COCO 2017 Caption MiniTest <code>(coco2017_cap_test)</code></li>
</ul>
</li>
</ul>
</li>
<li>DOCVQA <code>(docvqa)</code>
<ul>
<li>DOCVQA Validation <code>(docvqa_val)</code></li>
<li>DOCVQA Test <code>(docvqa_test)</code></li>
</ul>
</li>
<li>Ferret <code>(ferret)</code></li>
<li>Flickr30K <code>(flickr30k)</code>
<ul>
<li>Ferret Test <code>(ferret_test)</code></li>
</ul>
</li>
<li>GQA <code>(gqa)</code></li>
<li>HallusionBenchmark <code>(hallusion_bench_image)</code></li>
<li>Infographic VQA <code>(info_vqa)</code>
<ul>
<li>Infographic VQA Validation <code>(info_vqa_val)</code></li>
<li>Infographic VQA Test <code>(info_vqa_test)</code></li>
</ul>
</li>
<li>LLaVA-Bench <code>(llava_bench_wild)</code></li>
<li>LLaVA-Bench-COCO <code>(llava_bench_coco)</code></li>
<li>MathVista <code>(mathvista)</code>
<ul>
<li>MathVista Validation <code>(mathvista_testmini)</code></li>
<li>MathVista Test <code>(mathvista_test)</code></li>
</ul>
</li>
<li>MMBench <code>(mmbench)</code>
<ul>
<li>MMBench English <code>(mmbench_en)</code>
<ul>
<li>MMBench English Dev <code>(mmbench_en_dev)</code></li>
<li>MMBench English Test <code>(mmbench_en_test)</code></li>
</ul>
</li>
<li>MMBench Chinese <code>(mmbench_cn)</code>
<ul>
<li>MMBench Chinese Dev <code>(mmbench_cn_dev)</code></li>
<li>MMBench Chinese Test <code>(mmbench_cn_test)</code></li>
</ul>
</li>
</ul>
</li>
<li>MME <code>(mme)</code></li>
<li>MMMU <code>(mmmu)</code>
<ul>
<li>MMMU Validation <code>(mmmu_val)</code></li>
<li>MMMU Test <code>(mmmu_test)</code></li>
</ul>
</li>
<li>MMVet <code>(mmvet)</code></li>
<li>Multi-DocVQA <code>(multidocvqa)</code>
<ul>
<li>Multi-DocVQA Validation <code>(multidocvqa_val)</code></li>
<li>Multi-DocVQA Test <code>(multidocvqa_test)</code></li>
</ul>
</li>
</ul>
<p>&hellip; and more.</p>
<h2 id="citations">Citations</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bib" data-lang="bib"><span style="display:flex;"><span><span style="color:#a6e22e">@misc</span>{lmms_eval2024,
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">title</span>=<span style="color:#e6db74">{LMMs-Eval: Accelerating the Development of Large Multimoal Models}</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">url</span>=<span style="color:#e6db74">{https://github.com/EvolvingLMMs-Lab/lmms-eval}</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">author</span>=<span style="color:#e6db74">{Bo Li*, Peiyuan Zhang*, Kaicheng Zhang*, Fanyi Pu*, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li and Ziwei Liu}</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">publisher</span>    = <span style="color:#e6db74">{Zenodo}</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">version</span>      = <span style="color:#e6db74">{v0.1.0}</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">month</span>=<span style="color:#e6db74">{March}</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">year</span>=<span style="color:#e6db74">{2024}</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
  </div>
</article>

    
<script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>

<script src="//localhost:1313/js/darkmode.js" defer></script>
<footer class="bg-gray-900">
    <div class="max-w-md px-4 py-12 mx-auto overflow-hidden sm:max-w-3xl sm:px-6 lg:max-w-7xl lg:px-8">
        <nav class="flex flex-wrap justify-center -mx-5 -my-2" aria-label="Footer">
            
            <div class="px-5 py-2">
                <a href="//localhost:1313/team/" class="text-base text-gray-400 hover:text-gray-300">Team</a>
            </div>
            
            <div class="px-5 py-2">
                <a href="//localhost:1313/news/" class="text-base text-gray-400 hover:text-gray-300">News</a>
            </div>
            
            <div class="px-5 py-2">
                <a href="//localhost:1313/categories/blog/" class="text-base text-gray-400 hover:text-gray-300">Blog</a>
            </div>
            
        </nav>
        <div class="flex justify-center mt-8 space-x-6">
            
            <a href="#" class="text-gray-400 hover:text-gray-300">
                <span class="sr-only">Facebook</span>
                <svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                    <path fill-rule="evenodd"
                          d="M22 12c0-5.523-4.477-10-10-10S2 6.477 2 12c0 4.991 3.657 9.128 8.438 9.878v-6.987h-2.54V12h2.54V9.797c0-2.506 1.492-3.89 3.777-3.89 1.094 0 2.238.195 2.238.195v2.46h-1.26c-1.243 0-1.63.771-1.63 1.562V12h2.773l-.443 2.89h-2.33v6.988C18.343 21.128 22 16.991 22 12z"
                          clip-rule="evenodd" />
                </svg>
            </a>
            
            
            <a href="#" class="text-gray-400 hover:text-gray-300">
                <span class="sr-only">Instagram</span>
                <svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                    <path fill-rule="evenodd"
                          d="M12.315 2c2.43 0 2.784.013 3.808.06 1.064.049 1.791.218 2.427.465a4.902 4.902 0 011.772 1.153 4.902 4.902 0 011.153 1.772c.247.636.416 1.363.465 2.427.048 1.067.06 1.407.06 4.123v.08c0 2.643-.012 2.987-.06 4.043-.049 1.064-.218 1.791-.465 2.427a4.902 4.902 0 01-1.153 1.772 4.902 4.902 0 01-1.772 1.153c-.636.247-1.363.416-2.427.465-1.067.048-1.407.06-4.123.06h-.08c-2.643 0-2.987-.012-4.043-.06-1.064-.049-1.791-.218-2.427-.465a4.902 4.902 0 01-1.772-1.153 4.902 4.902 0 01-1.153-1.772c-.247-.636-.416-1.363-.465-2.427-.047-1.024-.06-1.379-.06-3.808v-.63c0-2.43.013-2.784.06-3.808.049-1.064.218-1.791.465-2.427a4.902 4.902 0 011.153-1.772A4.902 4.902 0 015.45 2.525c.636-.247 1.363-.416 2.427-.465C8.901 2.013 9.256 2 11.685 2h.63zm-.081 1.802h-.468c-2.456 0-2.784.011-3.807.058-.975.045-1.504.207-1.857.344-.467.182-.8.398-1.15.748-.35.35-.566.683-.748 1.15-.137.353-.3.882-.344 1.857-.047 1.023-.058 1.351-.058 3.807v.468c0 2.456.011 2.784.058 3.807.045.975.207 1.504.344 1.857.182.466.399.8.748 1.15.35.35.683.566 1.15.748.353.137.882.3 1.857.344 1.054.048 1.37.058 4.041.058h.08c2.597 0 2.917-.01 3.96-.058.976-.045 1.505-.207 1.858-.344.466-.182.8-.398 1.15-.748.35-.35.566-.683.748-1.15.137-.353.3-.882.344-1.857.048-1.055.058-1.37.058-4.041v-.08c0-2.597-.01-2.917-.058-3.96-.045-.976-.207-1.505-.344-1.858a3.097 3.097 0 00-.748-1.15 3.098 3.098 0 00-1.15-.748c-.353-.137-.882-.3-1.857-.344-1.023-.047-1.351-.058-3.807-.058zM12 6.865a5.135 5.135 0 110 10.27 5.135 5.135 0 010-10.27zm0 1.802a3.333 3.333 0 100 6.666 3.333 3.333 0 000-6.666zm5.338-3.205a1.2 1.2 0 110 2.4 1.2 1.2 0 010-2.4z"
                          clip-rule="evenodd" />
                </svg>
            </a>
            
            
            <a href="#" class="text-gray-400 hover:text-gray-300">
                <span class="sr-only">Twitter</span>
                <svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                    <path
                          d="M8.29 20.251c7.547 0 11.675-6.253 11.675-11.675 0-.178 0-.355-.012-.53A8.348 8.348 0 0022 5.92a8.19 8.19 0 01-2.357.646 4.118 4.118 0 001.804-2.27 8.224 8.224 0 01-2.605.996 4.107 4.107 0 00-6.993 3.743 11.65 11.65 0 01-8.457-4.287 4.106 4.106 0 001.27 5.477A4.072 4.072 0 012.8 9.713v.052a4.105 4.105 0 003.292 4.022 4.095 4.095 0 01-1.853.07 4.108 4.108 0 003.834 2.85A8.233 8.233 0 012 18.407a11.616 11.616 0 006.29 1.84" />
                </svg>
            </a>
            
            
            <a href="#" class="text-gray-400 hover:text-gray-300">
                <span class="sr-only">GitHub</span>
                <svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                    <path fill-rule="evenodd"
                          d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z"
                          clip-rule="evenodd" />
                </svg>
            </a>
            
        </div>
        <p class="mt-8 text-base text-center text-gray-400">&copy; 2024
            LMMs-Lab. All rights
            reserved.</p>
        <p class="mt-2 text-base text-center text-gray-400">Made with &#x2764;&#xfe0f; by <a
               href="https://nusserstudios.com" class="hover:underline hover:text-primary-600"><span
                      class="font-black uppercase">Nusser</span> <span class="font-light uppercase">Studios.</span></a>
        </p>
    </div>
</footer>
</body>

</html>